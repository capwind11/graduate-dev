{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import argparse\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import *\n",
    "import unsupervised.model as unsupervised_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_keys):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_keys)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, :, :])\n",
    "        return out\n",
    "def train(model,dataloader,current_epoch=0,num_epochs=10):\n",
    "    start_time = time.time()\n",
    "    for epoch in range(current_epoch,current_epoch+num_epochs):  # Loop over the dataset multiple times\n",
    "        train_loss = 0\n",
    "        for step, (seq, label) in enumerate(dataloader):\n",
    "            # Forward pass\n",
    "            seq = seq.clone().detach().view(-1, window_size, input_size).to(device)\n",
    "            label1= seq[:,1:,:].cpu().long()\n",
    "            label2 = label.view(-1,1,1)\n",
    "            label = torch.cat([label1,label2],1).view(-1,window_size)\n",
    "            label = label.reshape(label.size(0)*label.size(1))\n",
    "            output = model(seq)\n",
    "            output = output.reshape(output.size(0)*output.size(1),-1)\n",
    "            loss = criterion(output, label.to(device))\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            writer.add_graph(model, seq)\n",
    "        print('Epoch [{}/{}], train_loss: {:.4f}'.format(epoch + 1, current_epoch+num_epochs, train_loss / total_step))\n",
    "        writer.add_scalar('train_loss', train_loss / total_step, epoch + 1)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('elapsed_time: {:.3f}s'.format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_classes = 31\n",
    "num_epochs = 300\n",
    "batch_size = 2048\n",
    "input_size = 1\n",
    "model_dir = 'model'\n",
    "log = 'dev_v1.2_batch_size={}_epoch={}'.format(str(batch_size), str(num_epochs))\n",
    "num_layers = 2\n",
    "hidden_size = 64\n",
    "window_size = 10\n",
    "file_dir = '.\\\\data\\\\lstm\\\\dataset_official\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功加载模型model/dev_v1.2_batch_size=2048_epoch=300.pt\n"
     ]
    }
   ],
   "source": [
    "model = Model(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "if os.path.exists(model_dir + '/' + log + '.pt'):\n",
    "    model.load_state_dict(torch.load(model_dir + '/' + log + '.pt'))\n",
    "    print(\"成功加载模型\"+model_dir + '/' + log + '.pt')\n",
    "else:\n",
    "    print(\"重新训练\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File datatrain.csv does not exist: 'datatrain.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-ef72dd0e79bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_train_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_dir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'log/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Loss and optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\python_workspace\\jupyter_notebook\\毕业设计\\dev\\end_to_end_dev_v0\\utils\\data_utils.py\u001b[0m in \u001b[0;36mgenerate_train_data\u001b[1;34m(name, window_size)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_filter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[1;31m# blockId_list= data['BlockId'].tolist()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mseqs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'EventSequence'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Programs\\Anaconda\\envs\\cuda_pytorch\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Programs\\Anaconda\\envs\\cuda_pytorch\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Programs\\Anaconda\\envs\\cuda_pytorch\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Programs\\Anaconda\\envs\\cuda_pytorch\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Programs\\Anaconda\\envs\\cuda_pytorch\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File datatrain.csv does not exist: 'datatrain.csv'"
     ]
    }
   ],
   "source": [
    "train_dataset = generate_train_data(file_dir+'train.csv')\n",
    "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "writer = SummaryWriter(log_dir='log/' + log)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "# Train the model\n",
    "total_step = len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], train_loss: 0.4202\n",
      "Epoch [2/10], train_loss: 0.4119\n",
      "Epoch [3/10], train_loss: 0.4112\n",
      "Epoch [4/10], train_loss: 0.4108\n",
      "Epoch [5/10], train_loss: 0.4105\n",
      "Epoch [6/10], train_loss: 0.4103\n",
      "Epoch [7/10], train_loss: 0.4105\n",
      "Epoch [8/10], train_loss: 0.4099\n",
      "Epoch [9/10], train_loss: 0.4099\n",
      "Epoch [10/10], train_loss: 0.4101\n",
      "elapsed_time: 84.350s\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# model = Model(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "# if os.path.exists(model_dir + '/' + log + '.pt'):\n",
    "#     model.load_state_dict(torch.load(model_dir + '/' + log + '.pt'))\n",
    "#     print(\"成功加载模型\"+model_dir + '/' + log + '.pt')\n",
    "# else:\n",
    "#     print(\"重新训练\")\n",
    "model.train()\n",
    "train(model,dataloader,current_epoch=0,num_epochs=10)\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "torch.save(model.state_dict(), model_dir + '/' + log + '.pt')\n",
    "writer.close()\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dev_v1.2_batch_size=2048_epoch=300'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "torch.save(model.state_dict(), model_dir + '/' + log + '.pt')\n",
    "writer.close()\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简单检测一下训练的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-9cfba60386d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnum_of_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"测试对下一标签预测准确率\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m# Forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mseq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "num_of_seq = 0\n",
    "for step, (seq, label) in tqdm(enumerate(dataloader),desc=\"测试对下一标签预测准确率\"):\n",
    "    # Forward pass\n",
    "    seq = seq.clone().detach().view(-1, window_size, input_size).to(device)\n",
    "    label1= seq[:,1:,:].cpu().long()\n",
    "    label2 = label.view(-1,1,1)\n",
    "    label = torch.cat([label1,label2],1).view(-1,window_size)\n",
    "    label = label.reshape(label.size(0)*label.size(1))\n",
    "    output = model(seq)\n",
    "    output = output.reshape(output.size(0)*output.size(1),-1)\n",
    "    predicted = torch.argsort(output, 1)[:, -3:].cpu()\n",
    "    num_of_seq+=len(label)\n",
    "    for i in range(len(label)):\n",
    "\n",
    "    #     print(label[i],predicted[i])\n",
    "        if label[i] in predicted[i]:\n",
    "    #         print(label[i],predicted[i])\n",
    "            correct+=1   \n",
    "#             if label[i] == 30:\n",
    "#                 print(label[i-10:i+1])\n",
    "print('对下一标签预测准确率为: '+str(correct/num_of_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  4., 21.,  4.,  4., 10.,  8., 10.,  8., 25.,  8., 25., 25., 25.,\n",
      "        22., 22., 22., 20., 20., 20.,  0., 21.,  4.,  4.,  4., 25., 25., 10.,\n",
      "         8., 10., 21.,  4.,  4.,  4., 25., 25., 25., 10.,  8., 10.,  3.,  3.,\n",
      "         3.,  2., 22., 22., 22., 20., 20., 20.,  8., 10.,  8., 25., 25., 25.,\n",
      "        22., 22., 22., 20.,  4., 10.,  8., 10.,  8., 25., 25., 10.,  8., 25.],\n",
      "       device='cuda:0')\n",
      "tensor([ 4, 21,  4,  4, 10,  8, 10,  8, 25, 25, 25, 25, 25, 22, 22, 22, 20, 20,\n",
      "        20, 30, 21,  4,  4,  4, 25, 25, 10,  8, 10,  8,  4,  4,  4, 25, 25, 25,\n",
      "        10,  8, 10,  8,  3,  3,  2, 22, 22, 22, 20, 20, 20, 30, 10,  8, 25, 25,\n",
      "        25, 22, 22, 22, 20, 20, 10,  8, 10,  8, 25, 25, 10,  8, 25,  1])\n"
     ]
    }
   ],
   "source": [
    "print(seq.reshape(1,-1)[0][70:140])\n",
    "print(label.reshape(1,-1)[0][70:140])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 清理缓存释放空间 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (lstm): LSTM(1, 64, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=64, out_features=31, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(input_size, hidden_size, num_layers, num_classes)\n",
    "model.load_state_dict(torch.load(model_dir + '/' + log + '.pt'))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsupervised.predictor import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\\data\\lstm\\dataset_official\\normal.csv: 100%|████████████████████████████████| 14200/14200 [00:01<00:00, 12600.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sessions(.\\data\\lstm\\dataset_official\\normal.csv): 14200\n",
      "Number of seqs(.\\data\\lstm\\dataset_official\\normal.csv): 269989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\\data\\lstm\\dataset_official\\abnormal.csv: 100%|████████████████████████████████| 4123/4123 [00:00<00:00, 10008.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sessions(.\\data\\lstm\\dataset_official\\abnormal.csv): 4123\n",
      "Number of seqs(.\\data\\lstm\\dataset_official\\abnormal.csv): 88410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "batch_size = 20000\n",
    "window_size = 10\n",
    "test_normal_session, test_normal_dataset,normal_block = generate_predicted_data(file_dir+'normal.csv',window_size)\n",
    "normal_dataloader = DataLoader(test_normal_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "test_abnormal_session, test_abnormal_dataset,abnormal_block = generate_predicted_data(file_dir+'abnormal.csv',window_size)\n",
    "abnormal_dataloader = DataLoader(test_abnormal_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (lstm): LSTM(1, 64, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=64, out_features=31, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading data: 14it [01:04,  4.58s/it]\n",
      "loading data: 5it [00:20,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed_time: 84.812s\n",
      "false positive (FP): 415, false negative (FN): 1561, Precision: 86.060%, Recall: 62.139%, F1-measure: 72.169%\n",
      "Finished Predicting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_normal_result,test_abnormal_result = count_metries(model, normal_dataloader, abnormal_dataloader,\n",
    "                                                        test_normal_session, test_abnormal_session,\n",
    "                                                        10, window_size,ts=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normal:: 100%|████████████████████████████████████████████████████████████████| 14177/14177 [00:00<00:00, 25271.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sessions(hdfs_test_normal): 14177\n",
      "Number of seqs(hdfs_test_normal): 340455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normal:: 100%|██████████████████████████████████████████████████████████████████| 4123/4123 [00:00<00:00, 27493.86it/s]\n",
      "normal: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sessions(hdfs_test_abnormal): 4123\n",
      "Number of seqs(hdfs_test_abnormal): 108981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normal: 35it [00:39,  1.12s/it]\n",
      "abnormal: 11it [00:12,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed_time: 51.352s\n",
      "false positive (FP): 1541, false negative (FN): 359, Precision: 70.952%, Recall: 91.293%, F1-measure: 79.847%\n",
      "Finished Predicting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10000\n",
    "window_size = 5\n",
    "test_normal_session, test_normal_dataset, test_normal_seq,test_normal_label = generate_test_data('hdfs_test_normal',window_size)\n",
    "normal_dataloader = DataLoader(test_normal_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "test_abnormal_session, test_abnormal_dataset,test_abnormal_seq,test_abnormal_label = generate_test_data('hdfs_test_abnormal',window_size)\n",
    "abnormal_dataloader = DataLoader(test_abnormal_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "test_normal_result,test_abnormal_result = fast_predict(model,normal_dataloader,abnormal_dataloader,10,window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 快速预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fast predict\n",
    "def fast_predict(model,normal_dataloader,abnormal_dataloader,num_candidates=5,window_size=10):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    # Test the model\n",
    "    start_time = time.time()\n",
    "    test_normal_result = []\n",
    "    test_abnormal_result = []\n",
    "    with torch.no_grad():\n",
    "        result = []\n",
    "        with torch.no_grad():\n",
    "            for step, (seq, labels) in tqdm(enumerate(normal_dataloader), desc='normal'):\n",
    "                seq = seq.clone().detach().view(-1, window_size, input_size).to(device)\n",
    "                output = model(seq).cpu()\n",
    "\n",
    "                predicted = torch.argsort(output[:,-1,:], 1)[:,-num_candidates:]\n",
    "                for i, label in enumerate(labels):\n",
    "                    if label not in predicted[i]:\n",
    "                        test_normal_result.append(True)\n",
    "                    else:\n",
    "                        test_normal_result.append(False)\n",
    "    for session in test_normal_session:\n",
    "        for seq_id in session:\n",
    "            if test_normal_result[seq_id] == True:\n",
    "                FP += 1\n",
    "                break\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, (seq, labels) in tqdm(enumerate(abnormal_dataloader), desc='abnormal'):\n",
    "            seq = seq.clone().detach().view(-1, window_size, input_size).to(device)\n",
    "            output = model(seq).cpu()\n",
    "\n",
    "            predicted = torch.argsort(output[:,-1,:], 1)[:,-num_candidates:]\n",
    "            for i, label in enumerate(labels):\n",
    "                if label not in predicted[i]:\n",
    "                    test_abnormal_result.append(True)\n",
    "                else:\n",
    "                    test_abnormal_result.append(False)\n",
    "        for session in test_abnormal_session:\n",
    "            for seq_id in session:\n",
    "                if test_abnormal_result[seq_id] == True:\n",
    "                    TP += 1\n",
    "                    break\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('elapsed_time: {:.3f}s'.format(elapsed_time))\n",
    "    # Compute precision, recall and F1-measure\n",
    "    FN = len(test_abnormal_session) - TP\n",
    "    P = 100 * TP / (TP + FP)\n",
    "    R = 100 * TP / (TP + FN)\n",
    "    F1 = 2 * P * R / (P + R)\n",
    "    print('false positive (FP): {}, false negative (FN): {}, Precision: {:.3f}%, Recall: {:.3f}%, F1-measure: {:.3f}%'.format(FP, FN, P, R, F1))\n",
    "    print('Finished Predicting')\n",
    "    return test_normal_result,test_abnormal_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "normal: 0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "normal: 1it [00:01,  1.50s/it]\u001b[A\u001b[A\n",
      "\n",
      "normal: 2it [00:03,  1.59s/it]\u001b[A\u001b[A\n",
      "\n",
      "normal: 3it [00:05,  1.64s/it]\u001b[A\u001b[A\n",
      "\n",
      "normal: 4it [00:06,  1.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "normal: 5it [00:08,  1.72s/it]\u001b[A\u001b[A\n",
      "\n",
      "normal: 6it [00:10,  1.66s/it]\u001b[A\u001b[A\n",
      "\n",
      "normal: 7it [00:11,  1.67s/it]\u001b[A\u001b[A\n",
      "\n",
      "normal: 8it [00:13,  1.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "normal: 9it [00:14,  1.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "normal: 10it [00:16,  1.69s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 5, 1]' is invalid for input of size 53886",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-20db1cccaecc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabnormal_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'normal'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mseq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[-1, 5, 1]' is invalid for input of size 53886"
     ]
    }
   ],
   "source": [
    "TP = 0\n",
    "FP = 0\n",
    "# Test the model\n",
    "start_time = time.time()\n",
    "test_normal_result = []\n",
    "test_abnormal_result = []\n",
    "with torch.no_grad():\n",
    "    for step, (seq, labels) in tqdm(enumerate(abnormal_dataloader), desc='abnormal'):\n",
    "        seq = seq.clone().detach().view(-1, 5, input_size).to(device)\n",
    "        output = model(seq).cpu()\n",
    "\n",
    "        predicted = torch.argsort(output[:,-1,:], 1)[:,-5:]\n",
    "        for i, label in enumerate(labels):\n",
    "            if label not in predicted[i]:\n",
    "                test_abnormal_result.append(True)\n",
    "            else:\n",
    "                test_abnormal_result.append(False)\n",
    "    for session in test_abnormal_session:\n",
    "        for seq_id in session:\n",
    "            if test_abnormal_result[seq_id] == True:\n",
    "                TP += 1\n",
    "                break\n",
    "elapsed_time = time.time() - start_time\n",
    "print('elapsed_time: {:.3f}s'.format(elapsed_time))\n",
    "# Compute precision, recall and F1-measure\n",
    "FN = len(test_abnormal_session) - TP\n",
    "P = 100 * TP / (TP + FP)\n",
    "R = 100 * TP / (TP + FN)\n",
    "F1 = 2 * P * R / (P + R)\n",
    "print('false positive (FP): {}, false negative (FN): {}, Precision: {:.3f}%, Recall: {:.3f}%, F1-measure: {:.3f}%'.format(FP, FN, P, R, F1))\n",
    "print('Finished Predicting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8981, 6])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "normal: 0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 1it [00:01,  1.78s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 2it [00:03,  1.65s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 3it [00:04,  1.66s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 4it [00:06,  1.58s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 5it [00:07,  1.60s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 6it [00:09,  1.52s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 7it [00:10,  1.47s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 8it [00:12,  1.55s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 9it [00:13,  1.51s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 10it [00:15,  1.50s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 11it [00:16,  1.54s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 12it [00:18,  1.48s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 13it [00:19,  1.53s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 14it [00:21,  1.48s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 15it [00:22,  1.46s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 16it [00:24,  1.56s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 17it [00:25,  1.53s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 18it [00:27,  1.64s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 19it [00:29,  1.63s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 20it [00:30,  1.61s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 21it [00:32,  1.73s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 22it [00:34,  1.73s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 23it [00:36,  1.67s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 24it [00:37,  1.70s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 25it [00:39,  1.61s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 26it [00:41,  1.67s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 27it [00:42,  1.58s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 28it [00:43,  1.53s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 29it [00:45,  1.56s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 30it [00:46,  1.53s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 31it [00:48,  1.57s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 32it [00:50,  1.76s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 33it [00:53,  1.96s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 34it [00:55,  1.95s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 35it [00:55,  1.58s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "normal: 0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 1it [00:02,  2.07s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 2it [00:03,  1.87s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 3it [00:04,  1.64s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 4it [00:05,  1.48s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 5it [00:07,  1.44s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 6it [00:08,  1.34s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 7it [00:09,  1.26s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 8it [00:10,  1.29s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 9it [00:11,  1.24s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 10it [00:12,  1.20s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 11it [00:13,  1.27s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed_time: 69.522s\n",
      "false positive (FP): 1541, false negative (FN): 359, Precision: 70.952%, Recall: 91.293%, F1-measure: 79.847%\n",
      "Finished Predicting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  ...],\n",
       " [False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  ...])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_predict(model,normal_dataloader,abnormal_dataloader,10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "normal: 0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 1it [00:02,  2.16s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 2it [00:04,  2.22s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 3it [00:06,  2.06s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 4it [00:07,  1.88s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 5it [00:09,  1.89s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 6it [00:10,  1.74s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 7it [00:12,  1.64s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 8it [00:13,  1.63s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 9it [00:15,  1.56s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 10it [00:16,  1.57s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 11it [00:18,  1.58s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 12it [00:19,  1.52s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 13it [00:21,  1.55s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 14it [00:22,  1.50s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 15it [00:24,  1.46s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 16it [00:26,  1.52s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 17it [00:27,  1.48s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 18it [00:29,  1.53s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 19it [00:30,  1.48s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 20it [00:31,  1.46s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 21it [00:33,  1.51s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 22it [00:34,  1.48s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 23it [00:36,  1.45s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 24it [00:37,  1.50s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 25it [00:39,  1.46s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 26it [00:40,  1.51s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 27it [00:42,  1.49s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 28it [00:43,  1.45s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 29it [00:45,  1.51s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 30it [00:46,  1.47s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 31it [00:48,  1.52s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 32it [00:49,  1.48s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 33it [00:51,  1.46s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 35it [00:52,  1.51s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "normal: 0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 1it [00:01,  1.39s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 2it [00:03,  1.47s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 3it [00:04,  1.42s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 4it [00:05,  1.40s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 5it [00:07,  1.46s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 6it [00:08,  1.43s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 7it [00:10,  1.43s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 8it [00:11,  1.48s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 9it [00:13,  1.44s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 10it [00:14,  1.41s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "normal: 11it [00:15,  1.44s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed_time: 68.889s\n",
      "false positive (FP): 1541, false negative (FN): 359, Precision: 70.952%, Recall: 91.293%, F1-measure: 79.847%\n",
      "Finished Predicting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_normal_result,test_abnormal_result = fast_predict(model,normal_dataloader,abnormal_dataloader,10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4, 3, 4, 3, 23, 23, 23, 21, 21, 20)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_normal_seq[FP_result[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26, 26, 11, 9, 26, 23, 23, 23, 21, 21, 2)\n",
      "预测的序号排序: tensor([11, 18,  9, 23, 25,  4,  6,  5, 30, 21])\n",
      "对应的可能性: tensor([1.2008e-05, 6.2610e-05, 6.4171e-05, 1.5609e-04, 1.9687e-04, 2.1495e-04,\n",
      "        2.5744e-04, 9.2612e-04, 3.7864e-03, 9.9431e-01],\n",
      "       grad_fn=<IndexBackward>)\n",
      "\n",
      "(9, 11, 9, 11, 9, 26, 26, 25, 18, 5, 26)\n",
      "预测的序号排序: tensor([18, 30,  5, 23,  2, 21,  3,  6,  4, 16])\n",
      "对应的可能性: tensor([0.0016, 0.0018, 0.0022, 0.0074, 0.0120, 0.0610, 0.0631, 0.0851, 0.1440,\n",
      "        0.6199], grad_fn=<IndexBackward>)\n",
      "\n",
      "(4, 3, 4, 3, 3, 23, 23, 23, 21, 21, 20)\n",
      "预测的序号排序: tensor([ 2,  9, 18, 23, 25,  6,  4,  5, 30, 21])\n",
      "对应的可能性: tensor([4.0952e-06, 2.1788e-05, 3.2698e-05, 1.0069e-04, 1.0320e-04, 1.3307e-04,\n",
      "        1.8714e-04, 2.4859e-04, 1.1386e-03, 9.9802e-01],\n",
      "       grad_fn=<IndexBackward>)\n",
      "\n",
      "(5, 5, 11, 9, 26, 11, 9, 11, 9, 26, 18)\n",
      "预测的序号排序: tensor([30, 25,  9, 21,  2,  3, 23,  4, 11, 26])\n",
      "对应的可能性: tensor([5.6737e-06, 6.3830e-06, 2.5268e-05, 3.5254e-05, 6.4369e-05, 6.6575e-05,\n",
      "        1.1359e-04, 1.2045e-04, 4.4903e-04, 9.9911e-01],\n",
      "       grad_fn=<IndexBackward>)\n",
      "\n",
      "(25, 18, 5, 16, 6, 26, 26, 21, 25, 5, 18)\n",
      "预测的序号排序: tensor([26, 21,  5,  2,  3, 11, 30,  4,  6, 16])\n",
      "对应的可能性: tensor([4.2887e-04, 5.9130e-04, 6.9202e-04, 8.1189e-04, 2.6935e-03, 5.1918e-03,\n",
      "        7.4944e-03, 1.0989e-02, 1.0733e-01, 8.6321e-01],\n",
      "       grad_fn=<IndexBackward>)\n",
      "\n",
      "(26, 25, 18, 5, 16, 6, 26, 26, 26, 21, 2)\n",
      "预测的序号排序: tensor([ 6,  4,  9, 11, 23, 21, 18,  5, 30, 25])\n",
      "对应的可能性: tensor([4.1273e-04, 1.7671e-03, 2.1457e-03, 3.1278e-03, 2.1973e-02, 2.7341e-02,\n",
      "        6.4945e-02, 9.0889e-02, 1.0011e-01, 6.8714e-01],\n",
      "       grad_fn=<IndexBackward>)\n",
      "\n",
      "(22, 5, 11, 9, 11, 9, 11, 9, 26, 26, 18)\n",
      "预测的序号排序: tensor([25, 30,  9, 21,  2,  3, 23,  4, 11, 26])\n",
      "对应的可能性: tensor([1.3379e-05, 1.6427e-05, 2.2544e-05, 3.4695e-05, 7.0259e-05, 7.4684e-05,\n",
      "        9.2925e-05, 9.3777e-05, 2.2836e-04, 9.9935e-01],\n",
      "       grad_fn=<IndexBackward>)\n",
      "\n",
      "(25, 18, 5, 6, 16, 26, 26, 21, 25, 5, 18)\n",
      "预测的序号排序: tensor([ 5,  2, 26, 21, 11,  3, 30,  4,  6, 16])\n",
      "对应的可能性: tensor([0.0008, 0.0011, 0.0017, 0.0032, 0.0066, 0.0122, 0.0283, 0.0286, 0.3081,\n",
      "        0.6077], grad_fn=<IndexBackward>)\n",
      "\n",
      "(9, 26, 26, 26, 23, 23, 23, 21, 21, 21, 2)\n",
      "预测的序号排序: tensor([18, 11, 23,  4,  5, 25,  6,  9, 21, 30])\n",
      "对应的可能性: tensor([2.8927e-06, 5.1945e-06, 1.1847e-05, 1.3195e-05, 1.3551e-05, 1.7536e-05,\n",
      "        1.9130e-04, 2.2981e-04, 7.5638e-04, 9.9875e-01],\n",
      "       grad_fn=<IndexBackward>)\n",
      "\n",
      "(4, 4, 3, 3, 23, 23, 23, 21, 21, 21, 20)\n",
      "预测的序号排序: tensor([18, 11,  5, 23, 25,  4,  9,  6, 21, 30])\n",
      "对应的可能性: tensor([5.2980e-06, 6.2992e-06, 1.9546e-05, 1.9556e-05, 3.0029e-05, 3.1384e-05,\n",
      "        3.1278e-04, 3.6623e-04, 2.5055e-03, 9.9670e-01],\n",
      "       grad_fn=<IndexBackward>)\n",
      "\n",
      "(9, 26, 26, 26, 25, 18, 5, 6, 26, 16, 26)\n",
      "预测的序号排序: tensor([11,  3,  4,  5, 21,  9, 23, 30,  6, 25])\n",
      "对应的可能性: tensor([0.0034, 0.0086, 0.0120, 0.0143, 0.0475, 0.0782, 0.1032, 0.1050, 0.1467,\n",
      "        0.4751], grad_fn=<IndexBackward>)\n",
      "\n",
      "(11, 9, 5, 11, 9, 5, 11, 9, 26, 26, 22)\n",
      "预测的序号排序: tensor([25, 18, 21,  4,  9, 11,  3,  2, 23, 26])\n",
      "对应的可能性: tensor([6.3902e-04, 6.6193e-04, 1.3087e-03, 1.3233e-03, 2.9110e-03, 6.8536e-03,\n",
      "        1.5435e-02, 2.3630e-02, 1.9822e-01, 7.4898e-01],\n",
      "       grad_fn=<IndexBackward>)\n",
      "\n",
      "(18, 25, 11, 9, 11, 9, 11, 9, 26, 26, 5)\n",
      "预测的序号排序: tensor([21, 18, 11, 25, 30,  2,  3,  4, 26, 23])\n",
      "对应的可能性: tensor([2.7275e-04, 3.9181e-04, 8.4748e-04, 2.9573e-03, 5.4999e-02, 6.2348e-02,\n",
      "        6.5277e-02, 7.9950e-02, 1.2387e-01, 6.0899e-01],\n",
      "       grad_fn=<IndexBackward>)\n",
      "\n",
      "(3, 3, 3, 4, 23, 23, 23, 21, 21, 21, 20)\n",
      "预测的序号排序: tensor([18, 11,  5, 23, 25,  4,  9,  6, 21, 30])\n",
      "对应的可能性: tensor([6.8397e-06, 7.0292e-06, 2.1760e-05, 2.3963e-05, 4.0629e-05, 4.6017e-05,\n",
      "        3.7782e-04, 4.4020e-04, 3.4204e-03, 9.9561e-01],\n",
      "       grad_fn=<IndexBackward>)\n",
      "\n",
      "(22, 11, 9, 11, 9, 26, 11, 9, 26, 25, 5)\n",
      "预测的序号排序: tensor([21, 11, 26, 18, 25,  2,  3,  4, 30, 23])\n",
      "对应的可能性: tensor([8.9138e-05, 3.0668e-04, 8.6515e-04, 1.1009e-03, 2.3865e-03, 3.6944e-02,\n",
      "        3.7691e-02, 4.2285e-02, 1.5168e-01, 7.2662e-01],\n",
      "       grad_fn=<IndexBackward>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in FP_result[10:25]:\n",
    "    seq = test_normal_seq[i]\n",
    "    t = torch.FloatTensor(seq[:-1]).reshape(1,-1)\n",
    "    max_len = 60\n",
    "    pattern = set()\n",
    "    t,predicted,output = generate_seq(t,1,10,1)\n",
    "    prob = softmax(output)\n",
    "    print(seq)\n",
    "#     print(t.int().cpu().numpy()[0])\n",
    "    print(\"预测的序号排序:\",end=' ')\n",
    "    print(predicted)\n",
    "    print(\"对应的可能性:\",end=' ')\n",
    "    print(prob[predicted])\n",
    "    print()\n",
    "    pattern.add(tuple(t.int().cpu().numpy()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(start,window_size=10,num_candidates=5,scope=None):\n",
    "    bg = start.size(1) \n",
    "    if scope==None:\n",
    "        scope=num_candidates\n",
    "    for i in range(bg,bg+window_size):\n",
    "#         start = torch.FloatTensor(start)\n",
    "        seq = start.clone().detach().view(-1, i, input_size).to(device)\n",
    "        output = model(seq).cpu()[:,-1,:]\n",
    "        output = output.reshape(-1)\n",
    "        predicted = torch.argsort(output)[-num_candidates:]\n",
    "        nxt = random.randint(1,scope)\n",
    "        start = torch.cat([start,predicted[-nxt].reshape(1,-1).float()],1)\n",
    "    return start,predicted,output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[20., 20., 20., 26.]])\n",
      "预测的序号排序: tensor([26,  4, 23, 18,  6, 25, 11, 21,  5,  9])\n",
      "对应的可能性: tensor([1.7118e-04, 2.0131e-04, 5.6693e-04, 7.6491e-04, 1.1165e-02, 3.5378e-02,\n",
      "        1.4293e-01, 1.6214e-01, 2.4460e-01, 4.0196e-01],\n",
      "       grad_fn=<IndexBackward>)\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([ 20,20,20]).reshape(1,-1)\n",
    "t,predicted,output = generate_seq(t,1,10)\n",
    "prob = softmax(output)\n",
    "print(t)\n",
    "#     print(t.int().cpu().numpy()[0])\n",
    "print(\"预测的序号排序:\",end=' ')\n",
    "print(predicted)\n",
    "print(\"对应的可能性:\",end=' ')\n",
    "print(prob[predicted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  5., 18.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2192e-04, 2.4864e-01, 7.5096e-01], grad_fn=<IndexBackward>)\n"
     ]
    }
   ],
   "source": [
    "prob = softmax(output)\n",
    "print(prob[predicted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 5]\n",
      "预测的序号排序: tensor([18, 22,  5])\n",
      "对应的可能性: tensor([0.0009, 0.2551, 0.7420], grad_fn=<IndexBackward>)\n",
      "\n",
      "[0 5 5]\n",
      "预测的序号排序: tensor([18, 22,  5])\n",
      "对应的可能性: tensor([1.2192e-04, 2.4864e-01, 7.5096e-01], grad_fn=<IndexBackward>)\n",
      "\n",
      "[0 5 5 5]\n",
      "预测的序号排序: tensor([11, 22,  5])\n",
      "对应的可能性: tensor([3.8608e-05, 3.4809e-01, 6.5181e-01], grad_fn=<IndexBackward>)\n",
      "\n",
      "[ 0  5  5  5 22]\n",
      "预测的序号排序: tensor([26,  5, 22])\n",
      "对应的可能性: tensor([0.0010, 0.0013, 0.9970], grad_fn=<IndexBackward>)\n",
      "\n",
      "[ 0  5  5  5 22 11]\n",
      "预测的序号排序: tensor([26,  9, 11])\n",
      "对应的可能性: tensor([0.0159, 0.0212, 0.9590], grad_fn=<IndexBackward>)\n",
      "\n",
      "[ 0  5  5  5 22 11  9]\n",
      "预测的序号排序: tensor([26, 11,  9])\n",
      "对应的可能性: tensor([4.3829e-04, 2.5918e-03, 9.9695e-01], grad_fn=<IndexBackward>)\n",
      "\n",
      "[ 0  5  5  5 22 11  9 11]\n",
      "预测的序号排序: tensor([ 9, 26, 11])\n",
      "对应的可能性: tensor([0.0077, 0.0478, 0.9439], grad_fn=<IndexBackward>)\n",
      "\n",
      "[ 0  5  5  5 22 11  9 11  9]\n",
      "预测的序号排序: tensor([26, 11,  9])\n",
      "对应的可能性: tensor([5.0201e-04, 3.3620e-03, 9.9612e-01], grad_fn=<IndexBackward>)\n",
      "\n",
      "[ 0  5  5  5 22 11  9 11  9 11]\n",
      "预测的序号排序: tensor([ 9, 26, 11])\n",
      "对应的可能性: tensor([0.0024, 0.1226, 0.8749], grad_fn=<IndexBackward>)\n",
      "\n",
      "[ 0  5  5  5 22 11  9 11  9 11  9]\n",
      "预测的序号排序: tensor([11, 26,  9])\n",
      "对应的可能性: tensor([2.7529e-04, 4.3636e-04, 9.9929e-01], grad_fn=<IndexBackward>)\n",
      "\n",
      "[ 0  5  5  5 22 11  9 11  9 11  9 26]\n",
      "预测的序号排序: tensor([ 2, 11, 26])\n",
      "对应的可能性: tensor([6.7779e-05, 1.3595e-04, 9.9970e-01], grad_fn=<IndexBackward>)\n",
      "\n",
      "[ 0  5  5  5 22 11  9 11  9 11  9 26 26]\n",
      "预测的序号排序: tensor([ 4, 11, 26])\n",
      "对应的可能性: tensor([1.3575e-05, 4.9616e-05, 9.9991e-01], grad_fn=<IndexBackward>)\n",
      "\n",
      "[ 0  5  5  5 22 11  9 11  9 11  9 26 26 26]\n",
      "预测的序号排序: tensor([23, 11, 26])\n",
      "对应的可能性: tensor([1.3897e-04, 3.1265e-04, 9.9909e-01], grad_fn=<IndexBackward>)\n",
      "\n",
      "[ 0  5  5  5 22 11  9 11  9 11  9 26 26 26 23]\n",
      "预测的序号排序: tensor([ 2, 30, 23])\n",
      "对应的可能性: tensor([0.0858, 0.2746, 0.4617], grad_fn=<IndexBackward>)\n",
      "\n",
      "[ 0  5  5  5 22 11  9 11  9 11  9 26 26 26 23 23]\n",
      "预测的序号排序: tensor([30, 18, 23])\n",
      "对应的可能性: tensor([0.0022, 0.0028, 0.9918], grad_fn=<IndexBackward>)\n",
      "\n",
      "[ 0  5  5  5 22 11  9 11  9 11  9 26 26 26 23 23 23]\n",
      "预测的序号排序: tensor([21, 18, 23])\n",
      "对应的可能性: tensor([0.0012, 0.0024, 0.9934], grad_fn=<IndexBackward>)\n",
      "\n",
      "[ 0  5  5  5 22 11  9 11  9 11  9 26 26 26 23 23 23 21]\n",
      "预测的序号排序: tensor([23,  5, 21])\n",
      "对应的可能性: tensor([4.8894e-04, 1.0820e-03, 9.9797e-01], grad_fn=<IndexBackward>)\n",
      "\n",
      "[ 0  5  5  5 22 11  9 11  9 11  9 26 26 26 23 23 23 21 21]\n",
      "预测的序号排序: tensor([ 4,  5, 21])\n",
      "对应的可能性: tensor([1.2914e-04, 2.8094e-04, 9.9934e-01], grad_fn=<IndexBackward>)\n",
      "\n",
      "[ 0  5  5  5 22 11  9 11  9 11  9 26 26 26 23 23 23 21 21 21]\n",
      "预测的序号排序: tensor([ 4, 30, 21])\n",
      "对应的可能性: tensor([7.5188e-04, 5.7734e-02, 9.3912e-01], grad_fn=<IndexBackward>)\n",
      "\n",
      "[ 0  5  5  5 22 11  9 11  9 11  9 26 26 26 23 23 23 21 21 21 30]\n",
      "预测的序号排序: tensor([ 9, 21, 30])\n",
      "对应的可能性: tensor([1.6640e-04, 1.8914e-03, 9.9769e-01], grad_fn=<IndexBackward>)\n",
      "\n",
      "[ 0  5  5  5 22 11  9 11  9 11  9 26 26 26 23 23 23 21 21 21 30]\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([0]).reshape(1,-1)\n",
    "max_len = 60\n",
    "pattern = set()\n",
    "while t.size(1)<max_len:\n",
    "    t,predicted,output = generate_seq(t,1,3,1)\n",
    "    prob = softmax(output)\n",
    "    print(t.int().cpu().numpy()[0])\n",
    "    print(\"预测的序号排序:\",end=' ')\n",
    "    print(predicted)\n",
    "    print(\"对应的可能性:\",end=' ')\n",
    "    print(prob[predicted])\n",
    "    print()\n",
    "    if 30 in t[0]:\n",
    "        break\n",
    "print(t.int().cpu().numpy()[0])\n",
    "pattern.add(tuple(t.int().cpu().numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 22  5 11  9  5 26 26 26 11 11 11  9 11  9  3  4  3  4  4  4  4  3  2\n",
      "  2  2 23 23 23 21 21 21 21 21 30]\n",
      "[ 0 22 11  9  5  5 26 11  9 11 11 11 26 11  4  3  3 23  2  3  4  2 23  2\n",
      "  4  3  3  2 23  2 23 23 23  5  6 16  6  3  2 23 23 23 21  5  6 26 21 21\n",
      " 25 30]\n",
      "[ 0 22 11  9 11  9 26 26 11  9 11 26  4  4  3  2 23  2  3  2  4  3  3  3\n",
      "  4 23 23 23 21  4  6 16 26 21 25 30]\n",
      "[ 0 22  5  5 11  9 26 26 26 11  9  9  9 11 23 30]\n",
      "[ 0  5 22 11  5 11  9 11  9 11  9 26  4  4  4  4  3  4  3  4  3  4  4  4\n",
      " 23 23  5  4  3  2 23  2  4  3  4 23 23 23 21 21 30]\n",
      "[ 0 22 11  5 11 11 11  9 26 26 11  9  4  4  3  2 23 23 23 23 21 21 21 30]\n",
      "[ 0  5  5  5 22 11  9 11 11 11  9 26 26 11 26  3  4  4  4  4  4  4  3  4\n",
      "  2  2 23  2  4  4  4 23 23 23 21  5  6 16  6 26 26 30]\n",
      "[ 0 22  5  5  5 26 11 26 26 11  9  9  9 11 11  3  4  3 23 23 23 21  5 21\n",
      " 21 21 21 30]\n",
      "[ 0  5  5  5 22 11  9 26 11 11 11 11 26 11  3  4  3  2  2  2  2 23  5 16\n",
      "  6 16 26 30]\n",
      "[ 0  5 22  5  5 26 11  9 26 11 11 11  9 26 26 23 23 23  5 21 30]\n"
     ]
    }
   ],
   "source": [
    "pattern = set()\n",
    "for i in range(10):\n",
    "    t = torch.FloatTensor([0]).reshape(1,-1)\n",
    "    max_len = 60\n",
    "    while t.size(1)<max_len:\n",
    "        t,predicted,output = generate_seq(t,1,2)\n",
    "        if 30 in t[0]:\n",
    "            break\n",
    "    print(t.int().cpu().numpy()[0])\n",
    "    pattern.add(tuple(t.int().cpu().numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normal: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8., 10.,  8., 10.,  8.,  1.,  2.,  2.,  3.,  2.])\n",
      "tensor([[ 8.,  8., 25.,  2.,  2., 22.,  1., 22.,  1.,  1.,  3.]])\n",
      "tensor([10.,  8., 10.,  8.,  1.,  2.,  2.,  3.,  2.])\n",
      "tensor([ 8., 25.,  2.,  2., 22.,  1., 22.,  1.,  1.,  3.])\n",
      "\n",
      "tensor([10.,  8., 10.,  8.,  1.,  2.,  2.,  3.,  2.,  3.])\n",
      "tensor([[10.,  3.,  1.,  3.,  3., 22., 22., 24., 22.,  5.,  5.]])\n",
      "tensor([ 8., 10.,  8.,  1.,  2.,  2.,  3.,  2.,  3.])\n",
      "tensor([ 3.,  1.,  3.,  3., 22., 22., 24., 22.,  5.,  5.])\n",
      "\n",
      "tensor([ 8., 10.,  8.,  1.,  2.,  2.,  3.,  2.,  3.,  2.])\n",
      "tensor([[ 8.,  2.,  2.,  3.,  2.,  3.,  1., 21.,  1., 22.,  2.]])\n",
      "tensor([10.,  8.,  1.,  2.,  2.,  3.,  2.,  3.,  2.])\n",
      "tensor([ 2.,  2.,  3.,  2.,  3.,  1., 21.,  1., 22.,  2.])\n",
      "\n",
      "tensor([10.,  8.,  1.,  2.,  2.,  3.,  2.,  3.,  2.,  2.])\n",
      "tensor([[10.,  8.,  2., 22., 25., 20., 22., 30., 30., 20., 30.]])\n",
      "tensor([8., 1., 2., 2., 3., 2., 3., 2., 2.])\n",
      "tensor([ 8.,  2., 22., 25., 20., 22., 30., 30., 20., 30.])\n",
      "\n",
      "tensor([8., 1., 2., 2., 3., 2., 3., 2., 2., 2.])\n",
      "tensor([[ 8., 25., 10.,  1.,  1.,  1., 22., 22., 22., 22.,  3.]])\n",
      "tensor([1., 2., 2., 3., 2., 3., 2., 2., 2.])\n",
      "tensor([25., 10.,  1.,  1.,  1., 22., 22., 22., 22.,  3.])\n",
      "\n",
      "tensor([1., 2., 2., 3., 2., 3., 2., 2., 2., 3.])\n",
      "tensor([[ 1., 21.,  3.,  5., 15., 22., 10.,  2.,  1.,  1., 22.]])\n",
      "tensor([2., 2., 3., 2., 3., 2., 2., 2., 3.])\n",
      "tensor([21.,  3.,  5., 15., 22., 10.,  2.,  1.,  1., 22.])\n",
      "\n",
      "tensor([2., 2., 3., 2., 3., 2., 2., 2., 3., 1.])\n",
      "tensor([[ 2., 21.,  3.,  1., 22., 24., 20., 22., 20., 20., 24.]])\n",
      "tensor([2., 3., 2., 3., 2., 2., 2., 3., 1.])\n",
      "tensor([21.,  3.,  1., 22., 24., 20., 22., 20., 20., 24.])\n",
      "\n",
      "tensor([ 2.,  3.,  2.,  3.,  2.,  2.,  2.,  3.,  1., 22.])\n",
      "tensor([[ 2., 10.,  2., 10.,  1., 22., 24., 22.,  3., 15.,  5.]])\n",
      "tensor([ 3.,  2.,  3.,  2.,  2.,  2.,  3.,  1., 22.])\n",
      "tensor([10.,  2., 10.,  1., 22., 24., 22.,  3., 15.,  5.])\n",
      "\n",
      "tensor([ 3.,  2.,  3.,  2.,  2.,  2.,  3.,  1., 22., 22.])\n",
      "tensor([[ 3.,  3.,  2.,  2.,  2.,  3.,  2.,  3., 22., 22., 24.]])\n",
      "tensor([ 2.,  3.,  2.,  2.,  2.,  3.,  1., 22., 22.])\n",
      "tensor([ 3.,  2.,  2.,  2.,  3.,  2.,  3., 22., 22., 24.])\n",
      "\n",
      "tensor([ 2.,  3.,  2.,  2.,  2.,  3.,  1., 22., 22., 22.])\n",
      "tensor([[ 2.,  3.,  1.,  1.,  2.,  1., 22., 22.,  2., 20., 30.]])\n",
      "tensor([ 3.,  2.,  2.,  2.,  3.,  1., 22., 22., 22.])\n",
      "tensor([ 3.,  1.,  1.,  2.,  1., 22., 22.,  2., 20., 30.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for step, (seq, labels) in tqdm(enumerate(normal_dataloader), desc='normal'):\n",
    "        break\n",
    "for s in seq[100:110]:\n",
    "    t = s[:1].reshape(1,-1)\n",
    "    res,_ = generate_seq(t)\n",
    "    print(s)\n",
    "    print(res)\n",
    "\n",
    "    print(s[t.size(1):])\n",
    "    print(res[0,t.size(1):])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提取路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_seq(candidates):\n",
    "    current_log = [0]\n",
    "    idx_list = []\n",
    "    cur = 0\n",
    "    seq = torch.FloatTensor(current_log).reshape(1,-1)\n",
    "    _,predicted,output = generate_seq(seq,1,3)\n",
    "#     predicted = torch.sort(predicted)\n",
    "    while 30!=predicted[-1]:\n",
    "        prob = softmax(output)\n",
    "        flag = True\n",
    "        for log in torch.flip(predicted,dims=[0]):\n",
    "            if prob[log]>0.2 and log in candidates[cur:]:\n",
    "                current_log.append(log.numpy().tolist())\n",
    "                cur =candidates.index(log,cur)\n",
    "#                 print(log)\n",
    "#                 print(torch.flip(predicted,dims=[0]))\n",
    "                idx_list.append(cur)\n",
    "                cur = cur+1\n",
    "                flag = False\n",
    "                break\n",
    "        if flag:\n",
    "            break\n",
    "        seq = torch.FloatTensor(current_log).reshape(1,-1)\n",
    "        _,predicted,output = generate_seq(seq,1,3)\n",
    "    return current_log,idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_eles_from_list(eles_list,idx_list):\n",
    "    for i in idx_list[::-1]:\n",
    "        eles_list.pop(i)\n",
    "    return eles_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_seq(seq1,seq2):\n",
    "    i1 = 0\n",
    "    i2 = 0\n",
    "    seq_mess = []\n",
    "    while i1<len(seq1) and i2<len(seq2):\n",
    "        if random.randint(0,9)<5:\n",
    "            seq_mess.append(seq1[i1])\n",
    "            i1+=1\n",
    "        else:\n",
    "            seq_mess.append(seq2[i2])\n",
    "            i2+=1\n",
    "    if i1<len(seq1):\n",
    "        seq_mess.extend(seq1[i1:])\n",
    "    if i2<len(seq2):\n",
    "        seq_mess.extend(seq2[i2:])\n",
    "    return seq_mess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 22, 5, 5, 5, 22, 5, 11, 5, 9, 26, 11, 26, 11, 9, 11, 9, 9, 11, 11, 9, 26, 9, 23, 23, 26, 23, 26, 21, 21, 26, 23, 23, 21, 23, 21, 21, 21]\n"
     ]
    }
   ],
   "source": [
    "seq1 = [5,5,5,22,11,9,11,9,11,9,26,26,26,23,23,23,21,21,21]\n",
    "seq2 = [22,5,5,5,26,26,11,9,11,9,11,9,26,23,23,23,21,21,21]\n",
    "seq_mess = merge_seq(seq1,seq2)\n",
    "print(seq_mess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 5, 5, 5, 22, 11, 9, 11, 9, 11, 9, 26, 26, 26, 23, 23, 23, 21, 21, 21]\n",
      "[22, 5, 5, 5, 26, 26, 11, 9, 11, 11, 9, 9, 23, 23, 23, 21, 21, 26, 21]\n"
     ]
    }
   ],
   "source": [
    "candidates = [i for i in seq_mess]\n",
    "res,idx_list = extract_seq(candidates)\n",
    "print(res)\n",
    "print(del_eles_from_list(candidates,idx_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 5, 5, 5]\n",
      "[22, 26, 26, 11, 9, 11, 11, 9, 9, 23, 23, 23, 21, 21, 26, 21]\n"
     ]
    }
   ],
   "source": [
    "res,idx_list = extract_seq(candidates)\n",
    "print(res)\n",
    "print(del_eles_from_list(candidates,idx_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_seq_v2(candidates):\n",
    "    current_log = [0,candidates[0]]\n",
    "    idx_list = [0]\n",
    "    cur = 1\n",
    "    seq = torch.FloatTensor(current_log).reshape(1,-1)\n",
    "    _,predicted,output = generate_seq(seq,1,3)\n",
    "#     predicted = torch.sort(predicted)\n",
    "    while 30!=predicted[-1]:\n",
    "        prob = softmax(output)\n",
    "        flag = True\n",
    "        for log in torch.flip(predicted,dims=[0]):\n",
    "            if prob[log]>0.2 and log in candidates[cur:]:\n",
    "                current_log.append(log.numpy().tolist())\n",
    "                cur =candidates.index(log,cur)\n",
    "#                 print(log)\n",
    "#                 print(torch.flip(predicted,dims=[0]))\n",
    "                idx_list.append(cur)\n",
    "                cur = cur+1\n",
    "                flag = False\n",
    "                break\n",
    "        if flag:\n",
    "            break\n",
    "        seq = torch.FloatTensor(current_log[-4:]).reshape(1,-1)\n",
    "        _,predicted,output = generate_seq(seq,1,3)\n",
    "    return current_log,idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 22, 5, 5, 5, 26, 26, 26]\n",
      "[5, 5, 22, 11, 5, 9, 11, 9, 11, 11, 9, 9, 11, 11, 9, 9, 26, 26, 26, 23, 23, 23, 23, 23, 21, 21, 21, 23, 21, 21, 21]\n"
     ]
    }
   ],
   "source": [
    "candidates = [i for i in seq_mess]\n",
    "res,idx_list = extract_seq_v2(candidates)\n",
    "print(res)\n",
    "print(del_eles_from_list(candidates,idx_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 5, 5, 5, 26, 26, 26]\n",
      "[11, 9, 11, 9, 23, 23, 23, 23, 23, 21, 21, 21, 23, 21, 21, 21]\n"
     ]
    }
   ],
   "source": [
    "res,idx_list = extract_seq_v2(candidates)\n",
    "print(res)\n",
    "print(del_eles_from_list(candidates,idx_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 22]\n",
      "[11, 9, 11, 9, 11, 11, 9, 9, 11, 11, 9, 9, 26, 26, 26, 23, 23, 23, 23, 23, 21, 21, 21, 23, 21, 21, 21]\n"
     ]
    }
   ],
   "source": [
    "res,idx_list = extract_seq_v2(candidates)\n",
    "print(res)\n",
    "print(del_eles_from_list(candidates,idx_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([26, 11,  9])\n",
      "tensor([0.0030, 0.0069, 0.9894], grad_fn=<IndexBackward>)\n"
     ]
    }
   ],
   "source": [
    "seq = torch.FloatTensor([9,11]).reshape(1,-1)\n",
    "_,predicted,output=generate_seq(seq,1,3)\n",
    "print(predicted)\n",
    "print(softmax(output)[predicted])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 发现并发结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_concorrent(seq):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = [0]\n",
    "_,predicted,output=generate_seq(seq,1,3)\n",
    "print(predicted)\n",
    "print(softmax(output)[predicted])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit ('cuda_pytorch': conda)",
   "language": "python",
   "name": "python36764bitcudapytorchconda3e33319a1fef4dc990a9d2f171216946"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
