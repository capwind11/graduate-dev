{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from drain import model_v1, evaluator\n",
    "from drain.optimizer import Optimizer\n",
    "from drain import Drain\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_settings = {\n",
    "    'HDFS': {\n",
    "        'log_file': 'HDFS/HDFS_2k.log',\n",
    "        'log_format': '<Date> <Time> <Pid> <Level> <Component>: <Content>',\n",
    "        'regex': [r'blk_-?\\d+', r'(\\d+\\.){3}\\d+(:\\d+)?'],\n",
    "        'st': 0.8,\n",
    "        'depth': 5\n",
    "        },\n",
    "\n",
    "    'Hadoop': {\n",
    "        'log_file': 'Hadoop/Hadoop_2k.log',\n",
    "        'log_format': '<Date> <Time> <Level> \\[<Process>\\] <Component>: <Content>',\n",
    "        'removeCol':[0,1],\n",
    "        'regex': [r'(\\d+\\.){3}\\d+'],\n",
    "        'st': 0.5,\n",
    "        'depth': 4        \n",
    "        },\n",
    "\n",
    "    'Spark': {\n",
    "        'log_file': 'Spark/Spark_2k.log',\n",
    "        'log_format': '<Date> <Time> <Level> <Component>: <Content>',\n",
    "        'removeCol': [0, 1],\n",
    "        'regex': [r'(\\d+\\.){3}\\d+', r'\\b[KGTM]?B\\b', r'([\\w-]+\\.){2,}[\\w-]+'],\n",
    "        'st': 0.5,\n",
    "        'depth': 4\n",
    "        },\n",
    "\n",
    "    'Zookeeper': {\n",
    "        'log_file': 'Zookeeper/Zookeeper_2k.log',\n",
    "        'log_format': '<Date> <Time> - <Level>  \\[<Node>:<Component>@<Id>\\] - <Content>',\n",
    "        'removeCol': [0, 1],\n",
    "        'regex': [r'(/|)(\\d+\\.){3}\\d+(:\\d+)?'],\n",
    "        'st': 0.5,\n",
    "        'depth': 4        \n",
    "        },\n",
    "\n",
    "    'BGL': {\n",
    "        'log_file': 'BGL/BGL_2k.log',\n",
    "        'log_format': '<Label> <Timestamp> <Date> <Node> <Time> <NodeRepeat> <Type> <Component> <Level> <Content>',\n",
    "        'removeCol': [0, 1,2,3,4,5],\n",
    "        'regex': [r'core\\.\\d+'],\n",
    "        'st': 0.5,\n",
    "        'depth': 4        \n",
    "        },\n",
    "\n",
    "    'HPC': {\n",
    "        'log_file': 'HPC/HPC_2k.log',\n",
    "        'log_format': '<LogId> <Node> <Component> <State> <Time> <Flag> <Content>',\n",
    "        'removeCol': [0, 1,4],\n",
    "        'regex': [r'=\\d+'],\n",
    "        'st': 0.5,\n",
    "        'depth': 4\n",
    "        },\n",
    "\n",
    "    'Thunderbird': {\n",
    "        'log_file': 'Thunderbird/Thunderbird_2k.log',\n",
    "        'removeCol': [0, 1,4],\n",
    "        'log_format': '<Label> <Timestamp> <Date> <User> <Month> <Day> <Time> <Location> <Component>(\\[<PID>\\])?: <Content>',\n",
    "        'regex': [r'(\\d+\\.){3}\\d+'],\n",
    "        'st': 0.5,\n",
    "        'depth': 4        \n",
    "        },\n",
    "\n",
    "    'Windows': {\n",
    "        'log_file': 'Windows/Windows_2k.log',\n",
    "        'log_format': '<Date> <Time>, <Level>                  <Component>    <Content>',\n",
    "        'regex': [r'0x.*?\\s'],\n",
    "        'st': 0.7,\n",
    "        'depth': 5      \n",
    "        },\n",
    "\n",
    "    'Linux': {\n",
    "        'log_file': 'Linux/Linux_2k.log',\n",
    "        'log_format': '<Month> <Date> <Time> <Level> <Component>(\\[<PID>\\])?: <Content>',\n",
    "        'regex': [r'(\\d+\\.){3}\\d+', r'\\d{2}:\\d{2}:\\d{2}'],\n",
    "        'st': 0.39,\n",
    "        'depth': 6        \n",
    "        },\n",
    "\n",
    "    'Andriod': {\n",
    "        'log_file': 'Andriod/Andriod_2k.log',\n",
    "        'log_format': '<Date> <Time>  <Pid>  <Tid> <Level> <Component>: <Content>',\n",
    "        'regex': [r'(/[\\w-]+)+', r'([\\w-]+\\.){2,}[\\w-]+', r'\\b(\\-?\\+?\\d+)\\b|\\b0[Xx][a-fA-F\\d]+\\b|\\b[a-fA-F\\d]{4,}\\b'],\n",
    "        'st': 0.2,\n",
    "        'depth': 6   \n",
    "        },\n",
    "\n",
    "    'HealthApp': {\n",
    "        'log_file': 'HealthApp/HealthApp_2k.log',\n",
    "        'log_format': '<Time>\\|<Component>\\|<Pid>\\|<Content>',\n",
    "        'regex': [],\n",
    "        'st': 0.2,\n",
    "        'depth': 4\n",
    "        },\n",
    "\n",
    "    'Apache': {\n",
    "        'log_file': 'Apache/Apache_2k.log',\n",
    "        'log_format': '\\[<Time>\\] \\[<Level>\\] <Content>',\n",
    "        'regex': [r'(\\d+\\.){3}\\d+'],\n",
    "        'st': 0.5,\n",
    "        'depth': 4        \n",
    "        },\n",
    "\n",
    "    'Proxifier': {\n",
    "        'log_file': 'Proxifier/Proxifier_2k.log',\n",
    "        'log_format': '\\[<Time>\\] <Program> - <Content>',\n",
    "        'regex': [r'<\\d+\\ssec', r'([\\w-]+\\.)+[\\w-]+(:\\d+)?', r'\\d{2}:\\d{2}(:\\d{2})*', r'[KGTM]B'],\n",
    "        'st': 0.6,\n",
    "        'depth': 3\n",
    "        },\n",
    "\n",
    "    'OpenSSH': {\n",
    "        'log_file': 'OpenSSH/OpenSSH_2k.log',\n",
    "        'log_format': '<Date> <Day> <Time> <Component> sshd\\[<Pid>\\]: <Content>',\n",
    "        'regex': [r'(\\d+\\.){3}\\d+', r'([\\w-]+\\.){2,}[\\w-]+'],\n",
    "        'st': 0.6,\n",
    "        'depth': 5   \n",
    "        },\n",
    "\n",
    "    'OpenStack': {\n",
    "        'log_file': 'OpenStack/OpenStack_2k.log',\n",
    "        'log_format': '<Logrecord> <Date> <Time> <Pid> <Level> <Component> \\[<ADDR>\\] <Content>',\n",
    "        'regex': [r'((\\d+\\.){3}\\d+,?)+', r'/.+?\\s', r'\\d+'],\n",
    "        'st': 0.5,\n",
    "        'depth': 5\n",
    "        },\n",
    "\n",
    "    'Mac': {\n",
    "        'log_file': 'Mac/Mac_2k.log',\n",
    "        'log_format': '<Month>  <Date> <Time> <User> <Component>\\[<PID>\\]( \\(<Address>\\))?: <Content>',\n",
    "        'regex': [r'([\\w-]+\\.){2,}[\\w-]+'],\n",
    "        'st': 0.7,\n",
    "        'depth': 6   \n",
    "        },\n",
    "}\n",
    "input_dir = 'data/logs/' # The input directory of log file\n",
    "output_dir = 'data/logs/result/' # The output directory of parsing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printClusters(logClusters):\n",
    "    for logCluster in logClusters:\n",
    "        print(\"eventId: \"+str(logCluster.eventId)+\" template: \"+' '.join(logCluster.logTemplate),end=' ')\n",
    "        print(\"parentNode: \" ,end=' ')\n",
    "        for node in logCluster.parentNode:\n",
    "            print(node.token,end=' ')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_settings = {\n",
    "    'HDFS':[8,0.7,0.8,1,0.9],\n",
    "    'Hadoop':[9,0.5,0.8,1,0.8],\n",
    "    'Spark':[9,0.6,0.8,1,1],\n",
    "    'Zookeeper':[9,0.6,0.8,1,0.9],\n",
    "    'BGL':[9,0.7,0.8,1,0.9],\n",
    "    'HPC':[8,0.6,0.8,1,0.7],\n",
    "    'Windows':[8,0.9,0.8,1,0.7],\n",
    "    'Andriod':[8,1,0.9,100,1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on HDFS ===\n",
      "build the prefix tree process takes 1.8409967422485352\n",
      "the number of log used to build the parser is  2000\n",
      "*********************************************\n",
      "优化前,聚类数:  14 Precision: 1.0000, Recall: 1.0000, F1_measure: 1.0000, Parsing_Accuracy: 1.0000\n",
      "合并子树优化,聚类数:  14 Precision: 1.0000, Recall: 1.0000, F1_measure: 1.0000, Parsing_Accuracy: 1.0000\n",
      "合并聚类优化,聚类数:  14 Precision: 1.0000, Recall: 1.0000, F1_measure: 1.0000, Parsing_Accuracy: 1.0000\n",
      "\n",
      "=== Evaluation on Hadoop ===\n",
      "build the prefix tree process takes 1.8610014915466309\n",
      "the number of log used to build the parser is  2000\n",
      "*********************************************\n",
      "优化前,聚类数:  142 Precision: 1.0000, Recall: 0.9993, F1_measure: 0.9996, Parsing_Accuracy: 0.9725\n",
      "合并子树优化,聚类数:  117 Precision: 1.0000, Recall: 0.9998, F1_measure: 0.9999, Parsing_Accuracy: 0.9845\n",
      "合并聚类优化,聚类数:  114 Precision: 1.0000, Recall: 0.9999, F1_measure: 1.0000, Parsing_Accuracy: 0.9870\n",
      "\n",
      "=== Evaluation on Spark ===\n",
      "build the prefix tree process takes 2.3020007610321045\n",
      "the number of log used to build the parser is  2000\n",
      "*********************************************\n",
      "优化前,聚类数:  61 Precision: 1.0000, Recall: 0.9983, F1_measure: 0.9991, Parsing_Accuracy: 0.9825\n",
      "合并子树优化,聚类数:  32 Precision: 1.0000, Recall: 1.0000, F1_measure: 1.0000, Parsing_Accuracy: 0.9975\n",
      "合并聚类优化,聚类数:  32 Precision: 1.0000, Recall: 1.0000, F1_measure: 1.0000, Parsing_Accuracy: 0.9975\n",
      "\n",
      "=== Evaluation on Zookeeper ===\n",
      "build the prefix tree process takes 1.8720171451568604\n",
      "the number of log used to build the parser is  2000\n",
      "*********************************************\n",
      "优化前,聚类数:  55 Precision: 0.9999, Recall: 0.9997, F1_measure: 0.9998, Parsing_Accuracy: 0.9875\n",
      "合并子树优化,聚类数:  50 Precision: 0.9998, Recall: 0.9998, F1_measure: 0.9998, Parsing_Accuracy: 0.9885\n",
      "合并聚类优化,聚类数:  50 Precision: 0.9998, Recall: 0.9998, F1_measure: 0.9998, Parsing_Accuracy: 0.9885\n",
      "\n",
      "=== Evaluation on BGL ===\n",
      "build the prefix tree process takes 1.5460102558135986\n",
      "the number of log used to build the parser is  2000\n",
      "*********************************************\n",
      "优化前,聚类数:  223 Precision: 0.9997, Recall: 0.9190, F1_measure: 0.9576, Parsing_Accuracy: 0.7900\n",
      "合并子树优化,聚类数:  125 Precision: 0.9997, Recall: 0.9997, F1_measure: 0.9997, Parsing_Accuracy: 0.9560\n",
      "合并聚类优化,聚类数:  122 Precision: 0.9994, Recall: 0.9998, F1_measure: 0.9996, Parsing_Accuracy: 0.9580\n",
      "\n",
      "=== Evaluation on HPC ===\n",
      "build the prefix tree process takes 2.012003183364868\n",
      "the number of log used to build the parser is  2000\n",
      "*********************************************\n",
      "优化前,聚类数:  62 Precision: 0.9889, Recall: 0.9955, F1_measure: 0.9922, Parsing_Accuracy: 0.8875\n",
      "合并子树优化,聚类数:  52 Precision: 0.9887, Recall: 0.9957, F1_measure: 0.9922, Parsing_Accuracy: 0.8875\n",
      "合并聚类优化,聚类数:  48 Precision: 0.9846, Recall: 0.9999, F1_measure: 0.9922, Parsing_Accuracy: 0.9055\n",
      "\n",
      "=== Evaluation on Windows ===\n",
      "build the prefix tree process takes 3.2420589923858643\n",
      "the number of log used to build the parser is  2000\n",
      "*********************************************\n",
      "优化前,聚类数:  688 Precision: 1.0000, Recall: 0.4114, F1_measure: 0.5830, Parsing_Accuracy: 0.4000\n",
      "合并子树优化,聚类数:  57 Precision: 1.0000, Recall: 0.9999, F1_measure: 1.0000, Parsing_Accuracy: 0.9960\n",
      "合并聚类优化,聚类数:  48 Precision: 1.0000, Recall: 1.0000, F1_measure: 1.0000, Parsing_Accuracy: 0.9970\n",
      "\n",
      "=== Evaluation on Andriod ===\n",
      "build the prefix tree process takes 2.346062183380127\n",
      "the number of log used to build the parser is  2000\n",
      "*********************************************\n",
      "优化前,聚类数:  190 Precision: 1.0000, Recall: 0.9843, F1_measure: 0.9921, Parsing_Accuracy: 0.9105\n",
      "合并子树优化,聚类数:  180 Precision: 0.9701, Recall: 0.9844, F1_measure: 0.9772, Parsing_Accuracy: 0.7980\n",
      "合并聚类优化,聚类数:  180 Precision: 0.9701, Recall: 0.9844, F1_measure: 0.9772, Parsing_Accuracy: 0.7980\n"
     ]
    }
   ],
   "source": [
    "for dataset,param in optimize_settings.items():\n",
    "    setting = benchmark_settings[dataset]\n",
    "    print('\\n=== Evaluation on %s ==='%dataset)\n",
    "    indir = os.path.join(input_dir, os.path.dirname(setting['log_file']))\n",
    "    log_file = os.path.basename(setting['log_file'])\n",
    "    parser1 = model_v1.Drain(log_format=setting['log_format'], rex=setting['regex'], \n",
    "                            depth=param[0], st=param[1])\n",
    "    parser1.fit(isReconstruct=True,inputFile=input_dir+setting['log_file'],\n",
    "               outputFile=output_dir+log_file + '_structured.csv')\n",
    "    print('优化前,聚类数: ',len(parser1.logClusters),end=' ')\n",
    "    F1_measure, accuracy = evaluator.evaluate(\n",
    "                       groundtruth=os.path.join(indir, log_file + '_structured.csv'),\n",
    "                       parsedresult=os.path.join(output_dir, log_file + '_structured.csv')\n",
    "                       )\n",
    "    opt = Optimizer()\n",
    "    opt.modify(method='merge_sub_tree',\n",
    "               resultFile=output_dir+log_file + '_structured.csv', \n",
    "               logparser = parser1,st=param[2],nst=param[3])\n",
    "    logClusters = parser1.logClusters\n",
    "    print('合并子树优化,聚类数: ',len(logClusters),end=' ')\n",
    "    F1_measure, accuracy = evaluator.evaluate(\n",
    "                       groundtruth=os.path.join(indir, log_file + '_structured.csv'),\n",
    "                       parsedresult=os.path.join(output_dir, log_file + '_structured.csv')\n",
    "                       )\n",
    "    opt.modify(method = 'LCS',resultFile=output_dir+log_file + '_structured.csv',\n",
    "               logparser=parser1,st = param[4])\n",
    "\n",
    "    print('合并聚类优化,聚类数: ',len(parser1.logClusters),end=' ')\n",
    "    F1_measure, accuracy = evaluator.evaluate(\n",
    "                           groundtruth=os.path.join(indir, log_file + '_structured.csv'),\n",
    "                           parsedresult=os.path.join(output_dir, log_file + '_structured.csv')\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'Mac'\n",
    "setting =benchmark_settings[dataset]\n",
    "indir = os.path.join(input_dir, os.path.dirname(setting['log_file']))\n",
    "log_file = os.path.basename(setting['log_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原Drain,聚类数:  578 Precision: 0.9759, Recall: 0.9750, F1_measure: 0.9755, Parsing_Accuracy: 0.7865\n",
      "真实聚类数:  341\n"
     ]
    }
   ],
   "source": [
    "parser = Drain.LogParser(log_format=setting['log_format'], indir=indir, outdir=output_dir, rex=setting['regex'],\n",
    "                         depth=setting['depth'], st=setting['st'])\n",
    "parser.parse(log_file)\n",
    "logClusters = parser.logCluL\n",
    "print('原Drain,聚类数: ', len(logClusters), end=' ')\n",
    "F1_measure, accuracy = evaluator.evaluate(\n",
    "    groundtruth=os.path.join(indir, log_file + '_structured.csv'),\n",
    "    parsedresult=os.path.join(output_dir, log_file + '_structured.csv')\n",
    ")\n",
    "df = pd.read_csv(os.path.join(indir,log_file + '_structured.csv'))\n",
    "print('真实聚类数: ', len(df['EventId'].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on Mac ===\n",
      "build the prefix tree process takes 2.846994400024414\n",
      "the number of log used to build the parser is  2000\n",
      "*********************************************\n",
      "优化前,聚类数:  551 Precision: 0.9830, Recall: 0.9267, F1_measure: 0.9540, Parsing_Accuracy: 0.6985\n",
      "合并子树优化,聚类数:  337 Precision: 0.9740, Recall: 0.9891, F1_measure: 0.9815, Parsing_Accuracy: 0.7775\n",
      "合并聚类优化,聚类数:  335 Precision: 0.9728, Recall: 0.9891, F1_measure: 0.9809, Parsing_Accuracy: 0.7700\n"
     ]
    }
   ],
   "source": [
    "print('\\n=== Evaluation on %s ==='%dataset)\n",
    "indir = os.path.join(input_dir, os.path.dirname(setting['log_file']))\n",
    "log_file = os.path.basename(setting['log_file'])\n",
    "parser1 = model_v1.Drain(log_format=setting['log_format'], rex=setting['regex'], \n",
    "                        depth=8, st=0.8)\n",
    "parser1.fit(isReconstruct=True,inputFile=input_dir+setting['log_file'],\n",
    "           outputFile=output_dir+log_file + '_structured.csv')\n",
    "print('优化前,聚类数: ',len(parser1.logClusters),end=' ')\n",
    "F1_measure, accuracy = evaluator.evaluate(\n",
    "                   groundtruth=os.path.join(indir, log_file + '_structured.csv'),\n",
    "                   parsedresult=os.path.join(output_dir, log_file + '_structured.csv')\n",
    "                   )\n",
    "opt = Optimizer()\n",
    "opt.modify(method='merge_sub_tree',\n",
    "           resultFile=output_dir+log_file + '_structured.csv', \n",
    "           logparser = parser1,st=0.8,nst=1)\n",
    "logClusters = parser1.logClusters\n",
    "print('合并子树优化,聚类数: ',len(logClusters),end=' ')\n",
    "F1_measure, accuracy = evaluator.evaluate(\n",
    "                   groundtruth=os.path.join(indir, log_file + '_structured.csv'),\n",
    "                   parsedresult=os.path.join(output_dir, log_file + '_structured.csv')\n",
    "                   )\n",
    "opt.modify(method = 'LCS',resultFile=output_dir+log_file + '_structured.csv',\n",
    "           logparser=parser1,st = 0.9)\n",
    "\n",
    "print('合并聚类优化,聚类数: ',len(parser1.logClusters),end=' ')\n",
    "F1_measure, accuracy = evaluator.evaluate(\n",
    "                       groundtruth=os.path.join(indir, log_file + '_structured.csv'),\n",
    "                       parsedresult=os.path.join(output_dir, log_file + '_structured.csv')\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eventId: 1 template: Notification time out: <*> parentNode:  * \n",
      "eventId: 2 template: Received connection request <*> parentNode:  <*> \n",
      "eventId: 3 template: Send worker leaving thread parentNode:  thread \n",
      "eventId: 4 template: Interrupted while waiting for message on queue parentNode:  queue \n",
      "eventId: 5 template: Connection broken for id <*> my id = <*> error = parentNode:  id \n",
      "eventId: 6 template: Interrupting SendWorker parentNode:  SendWorker \n",
      "eventId: 7 template: Closed socket connection for client <*> which had sessionid <*> parentNode:  which \n",
      "eventId: 8 template: caught end of stream exception parentNode:  exception \n",
      "eventId: 9 template: Client attempting to renew session <*> at <*> parentNode:  at \n",
      "eventId: 10 template: Client attempting to establish new session at <*> parentNode:  at \n",
      "eventId: 11 template: Established session <*> with negotiated timeout <*> for client <*> parentNode:  * \n",
      "eventId: 12 template: Accepted socket connection from <*> parentNode:  <*> \n",
      "eventId: 13 template: Unexpected Exception: parentNode:  Exception: \n",
      "eventId: 14 template: Connection request from old client <*>; will be dropped if server is in r-o mode parentNode:  will \n",
      "eventId: 15 template: Server environment:user.dir=/ parentNode:  environment:user.dir=/ \n",
      "eventId: 16 template: Cannot open channel to <*> at election address <*> parentNode:  election \n",
      "eventId: 17 template: Processed session termination for sessionid: <*> parentNode:  * \n",
      "eventId: 18 template: Expiring session <*> timeout of <*> exceeded parentNode:  exceeded \n",
      "eventId: 19 template: ******* GOODBYE <*> ******** parentNode:  ******** \n",
      "eventId: 20 template: LOOKING parentNode:  LOOKING \n",
      "eventId: 21 template: My election bind port: <*><*> parentNode:  <*><*> \n",
      "eventId: 22 template: Have smaller server identifier, so dropping the connection: (2, 1) parentNode:  the \n",
      "eventId: 23 template: FOLLOWING parentNode:  FOLLOWING \n",
      "eventId: 24 template: New election. My id = 1, proposed zxid=0x700000000 parentNode:  proposed \n",
      "eventId: 25 template: Server <*> parentNode:  * * * \n",
      "eventId: 26 template: FOLLOWING - LEADER ELECTION TOOK - <*> parentNode:  * \n",
      "eventId: 28 template: autopurge.snapRetainCount set to 3 parentNode:  * \n",
      "eventId: 29 template: Server environment:java.vendor=Oracle Corporation parentNode:  Corporation \n",
      "eventId: 30 template: Revalidating client: <*> parentNode:  * \n",
      "eventId: 31 template: Server environment:zookeeper.version=3.4.5--1, built on 06/10/2013 17:26 GMT parentNode:  GMT \n",
      "eventId: 32 template: Server environment:user.name=zookeeper parentNode:  environment:user.name=zookeeper \n",
      "eventId: 33 template: Server environment:os.name=Linux parentNode:  environment:os.name=Linux \n",
      "eventId: 34 template: Closed socket connection for client <*> (no session established for client) parentNode:  (no \n",
      "eventId: 35 template: Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running parentNode:  due \n",
      "eventId: 36 template: Notification: <*> (n.leader), <*> (n.zxid), <*> (n.round), LOOKING (n.state), <*> (n.sid), <*> (n.peerEPoch), <*> (my state) parentNode:  (n.round), (n.round), (n.round), \n",
      "eventId: 37 template: tickTime set to 2000 parentNode:  * \n",
      "eventId: 38 template: Unexpected exception causing shutdown while sock still open parentNode:  still \n",
      "eventId: 39 template: Got user-level KeeperException when processing sessionid:0x34ed93485090001 type:create cxid:0x55b8bb0f zxid:0x100000010 txntype:-1 reqpath:n/a Error Path:/home/curi/.zookeeper Error:KeeperErrorCode = NodeExists for /home/curi/.zookeeper parentNode:  type:create \n",
      "eventId: 40 template: Starting quorum peer parentNode:  peer \n",
      "eventId: 42 template: autopurge.purgeInterval set to 0 parentNode:  * \n",
      "eventId: 43 template: Snapshotting: <*> to <*> parentNode:  * * \n",
      "eventId: 44 template: Follower sid: 3 : info : org.apache.zookeeper.server.quorum.QuorumPeer$QuorumServer@33557fe4 parentNode:  * \n",
      "eventId: 45 template: maxSessionTimeout set to -1 parentNode:  * \n",
      "eventId: 47 template: Have quorum of supporters; starting up and setting last processed zxid: 0xb00000000 parentNode:  and \n",
      "eventId: 49 template: shutdown of request processor complete parentNode:  complete \n",
      "eventId: 50 template: First is 0x0 parentNode:  * \n",
      "eventId: 51 template: Reading snapshot /var/lib/zookeeper/version-2/snapshot.b00000084 parentNode:  * \n",
      "eventId: 52 template: Sending DIFF parentNode:  DIFF \n",
      "eventId: 53 template: Getting a snapshot from leader parentNode:  leader \n",
      "eventId: 54 template: minSessionTimeout set to -1 parentNode:  * \n"
     ]
    }
   ],
   "source": [
    "printClusters(parser1.logClusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合并子树优化,聚类数:  48 Precision: 0.9998, Recall: 0.9998, F1_measure: 0.9998, Parsing_Accuracy: 0.9885\n",
      "合并聚类优化,聚类数:  48 Precision: 0.9998, Recall: 0.9998, F1_measure: 0.9998, Parsing_Accuracy: 0.9885\n"
     ]
    }
   ],
   "source": [
    "opt = Optimizer()\n",
    "opt.modify(method='merge_sub_tree',\n",
    "           resultFile=output_dir+log_file + '_structured.csv', \n",
    "           logparser = parser1,nst=1)\n",
    "logClusters = parser1.logClusters\n",
    "print('合并子树优化,聚类数: ',len(logClusters),end=' ')\n",
    "F1_measure, accuracy = evaluator.evaluate(\n",
    "                   groundtruth=os.path.join(indir, log_file + '_structured.csv'),\n",
    "                   parsedresult=os.path.join(output_dir, log_file + '_structured.csv')\n",
    "                   )\n",
    "opt.modify(method = 'seq_dist',resultFile=output_dir+log_file + '_structured.csv',\n",
    "           logparser=parser1,st = 1)\n",
    "\n",
    "print('合并聚类优化,聚类数: ',len(parser1.logClusters),end=' ')\n",
    "F1_measure, accuracy = evaluator.evaluate(\n",
    "                       groundtruth=os.path.join(indir, log_file + '_structured.csv'),\n",
    "                       parsedresult=os.path.join(output_dir, log_file + '_structured.csv')\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eventId: 1 template: PacketResponder <*> for block <*> terminating parentNode:  block \n",
      "eventId: 2 template: BLOCK* NameSystem.addStoredBlock: blockMap updated: <*> is added to <*> size <*> parentNode:  updated: \n",
      "eventId: 3 template: Received block <*> of size <*> from /<*> parentNode:  of \n",
      "eventId: 4 template: <*> block <*> src: /<*> dest: /<*> parentNode:  src: src: \n",
      "eventId: 5 template: BLOCK* NameSystem.allocateBlock: <*> <*> parentNode:  <*> \n",
      "eventId: 6 template: Verification succeeded for <*> parentNode:  <*> \n",
      "eventId: 7 template: Deleting block <*> file <*> parentNode:  file \n",
      "eventId: 8 template: <*> Served block <*> to /<*> parentNode:  <*> \n",
      "eventId: 9 template: <*>:Got exception while serving <*> to /<*>: parentNode:  serving \n",
      "eventId: 10 template: BLOCK* NameSystem.delete: <*> is added to invalidSet of <*> parentNode:  is \n",
      "eventId: 11 template: <*> Starting thread to transfer block <*> to <*> parentNode:  to \n",
      "eventId: 12 template: BLOCK* ask <*> to <*> <*> parentNode:  to \n"
     ]
    }
   ],
   "source": [
    "printClusters(parser.logClusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PacketResponder <*> for block <*> terminating\n",
      "BLOCK* NameSystem.addStoredBlock: blockMap updated: <*> is added to <*> size <*>\n",
      "Received block <*> of size <*> from /<*>\n",
      "Receiving block <*> src: /<*> dest: /<*>\n",
      "BLOCK* NameSystem.allocateBlock: <*> <*>\n",
      "Verification succeeded for <*>\n",
      "Deleting block <*> file <*>\n",
      "<*> Served block <*> to /<*>\n",
      "<*>:Got exception while serving <*> to /<*>:\n",
      "BLOCK* NameSystem.delete: <*> is added to invalidSet of <*>\n",
      "<*> Starting thread to transfer block <*> to <*>\n",
      "BLOCK* ask <*> to delete <*>\n",
      "Received block <*> src: /<*> dest: /<*> of size 67108864\n",
      "BLOCK* ask <*> to delete <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*>\n",
      "BLOCK* ask <*> to delete <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*>\n",
      "BLOCK* ask <*> to replicate <*> to datanode(s) <*>\n",
      "BLOCK* ask <*> to delete <*> <*> <*> <*> <*> <*> <*> <*> <*>\n"
     ]
    }
   ],
   "source": [
    "for i in parser.logCluL:\n",
    "    print(' '.join(i.logTemplate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原Drain,聚类数:  12 Precision: 1.0000, Recall: 1.0000, F1_measure: 1.0000, Parsing_Accuracy: 0.9975\n",
      "真实聚类数:  14\n"
     ]
    }
   ],
   "source": [
    "parser = Drain.LogParser(log_format=setting['log_format'], indir=indir, \n",
    "                         outdir=output_dir, rex=setting['regex'], depth=4, st=0.5)\n",
    "parser.parse(log_file)\n",
    "\n",
    "print('原Drain,聚类数: ',len(logClusters),end=' ')\n",
    "F1_measure, accuracy = evaluator.evaluate(\n",
    "                       groundtruth=os.path.join(indir, log_file + '_structured.csv'),\n",
    "                       parsedresult=os.path.join(output_dir, log_file + '_structured.csv')\n",
    "                       )\n",
    "\n",
    "df = pd.read_csv(os.path.join(indir,log_file + '_structured.csv'))\n",
    "print('真实聚类数: ', len(df['EventId'].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on HDFS ===\n",
      "build the prefix tree process takes 2.0962367057800293\n",
      "the number of log used to build the parser is  2000\n",
      "*********************************************\n",
      "优化前,聚类数:  13 Precision: 1.0000, Recall: 1.0000, F1_measure: 1.0000, Parsing_Accuracy: 0.9970\n",
      "合并子树优化,聚类数:  13 Precision: 1.0000, Recall: 1.0000, F1_measure: 1.0000, Parsing_Accuracy: 0.9970\n",
      "合并聚类优化,聚类数:  12 Precision: 0.9977, Recall: 1.0000, F1_measure: 0.9988, Parsing_Accuracy: 0.8500\n",
      "原Drain,聚类数:  17 Precision: 1.0000, Recall: 1.0000, F1_measure: 1.0000, Parsing_Accuracy: 0.9975\n",
      "\n",
      "=== Evaluation on Hadoop ===\n",
      "\n",
      "=== Evaluation on Spark ===\n",
      "build the prefix tree process takes 1.6864142417907715\n",
      "the number of log used to build the parser is  2000\n",
      "*********************************************\n",
      "优化前,聚类数:  30 Precision: 0.9839, Recall: 1.0000, F1_measure: 0.9919, Parsing_Accuracy: 0.9220\n",
      "合并子树优化,聚类数:  28 Precision: 0.7263, Recall: 1.0000, F1_measure: 0.8415, Parsing_Accuracy: 0.6175\n",
      "合并聚类优化,聚类数:  28 Precision: 0.7263, Recall: 1.0000, F1_measure: 0.8415, Parsing_Accuracy: 0.6175\n",
      "原Drain,聚类数:  29 Precision: 0.9839, Recall: 1.0000, F1_measure: 0.9919, Parsing_Accuracy: 0.9200\n",
      "\n",
      "=== Evaluation on Zookeeper ===\n",
      "\n",
      "=== Evaluation on BGL ===\n",
      "\n",
      "=== Evaluation on HPC ===\n",
      "\n",
      "=== Evaluation on Thunderbird ===\n",
      "\n",
      "=== Evaluation on Windows ===\n",
      "build the prefix tree process takes 1.472032070159912\n",
      "the number of log used to build the parser is  2000\n",
      "*********************************************\n",
      "优化前,聚类数:  53 Precision: 1.0000, Recall: 1.0000, F1_measure: 1.0000, Parsing_Accuracy: 0.9970\n",
      "合并子树优化,聚类数:  49 Precision: 0.9986, Recall: 1.0000, F1_measure: 0.9993, Parsing_Accuracy: 0.6925\n",
      "合并聚类优化,聚类数:  49 Precision: 0.9986, Recall: 1.0000, F1_measure: 0.9993, Parsing_Accuracy: 0.6925\n",
      "原Drain,聚类数:  53 Precision: 1.0000, Recall: 1.0000, F1_measure: 1.0000, Parsing_Accuracy: 0.9970\n",
      "\n",
      "=== Evaluation on Linux ===\n",
      "\n",
      "=== Evaluation on Andriod ===\n",
      "\n",
      "=== Evaluation on HealthApp ===\n",
      "\n",
      "=== Evaluation on Apache ===\n",
      "build the prefix tree process takes 1.443925142288208\n",
      "the number of log used to build the parser is  2000\n",
      "*********************************************\n",
      "优化前,聚类数:  6 Precision: 1.0000, Recall: 1.0000, F1_measure: 1.0000, Parsing_Accuracy: 1.0000\n",
      "合并子树优化,聚类数:  6 Precision: 1.0000, Recall: 1.0000, F1_measure: 1.0000, Parsing_Accuracy: 1.0000\n",
      "合并聚类优化,聚类数:  6 Precision: 1.0000, Recall: 1.0000, F1_measure: 1.0000, Parsing_Accuracy: 1.0000\n",
      "原Drain,聚类数:  6 Precision: 1.0000, Recall: 1.0000, F1_measure: 1.0000, Parsing_Accuracy: 1.0000\n",
      "\n",
      "=== Evaluation on Proxifier ===\n",
      "build the prefix tree process takes 1.5749876499176025\n",
      "the number of log used to build the parser is  2000\n",
      "*********************************************\n",
      "优化前,聚类数:  8 Precision: 0.9944, Recall: 0.6480, F1_measure: 0.7847, Parsing_Accuracy: 0.0155\n",
      "合并子树优化,聚类数:  8 Precision: 0.9944, Recall: 0.6480, F1_measure: 0.7847, Parsing_Accuracy: 0.0155\n",
      "合并聚类优化,聚类数:  8 Precision: 0.9944, Recall: 0.6480, F1_measure: 0.7847, Parsing_Accuracy: 0.0155\n",
      "原Drain,聚类数:  18 Precision: 1.0000, Recall: 0.6459, F1_measure: 0.7849, Parsing_Accuracy: 0.5265\n",
      "\n",
      "=== Evaluation on OpenSSH ===\n",
      "build the prefix tree process takes 1.768998622894287\n",
      "the number of log used to build the parser is  2000\n",
      "*********************************************\n",
      "优化前,聚类数:  28 Precision: 0.8545, Recall: 0.9779, F1_measure: 0.9120, Parsing_Accuracy: 0.2315\n",
      "合并子树优化,聚类数:  26 Precision: 0.8531, Recall: 0.9787, F1_measure: 0.9116, Parsing_Accuracy: 0.2185\n",
      "合并聚类优化,聚类数:  21 Precision: 0.8557, Recall: 0.9992, F1_measure: 0.9219, Parsing_Accuracy: 0.4100\n",
      "原Drain,聚类数:  24 Precision: 0.9984, Recall: 1.0000, F1_measure: 0.9992, Parsing_Accuracy: 0.7875\n",
      "\n",
      "=== Evaluation on OpenStack ===\n",
      "build the prefix tree process takes 1.7650001049041748\n",
      "the number of log used to build the parser is  2000\n",
      "*********************************************\n",
      "优化前,聚类数:  469 Precision: 0.9997, Recall: 0.9855, F1_measure: 0.9925, Parsing_Accuracy: 0.7215\n",
      "合并子树优化,聚类数:  44 Precision: 0.8332, Recall: 0.9991, F1_measure: 0.9087, Parsing_Accuracy: 0.2265\n",
      "合并聚类优化,聚类数:  35 Precision: 0.8265, Recall: 0.9998, F1_measure: 0.9049, Parsing_Accuracy: 0.2480\n",
      "原Drain,聚类数:  299 Precision: 0.9992, Recall: 0.9860, F1_measure: 0.9925, Parsing_Accuracy: 0.7325\n",
      "\n",
      "=== Evaluation on Mac ===\n",
      "\n",
      "=== Overall evaluation results ===\n",
      "           F1_measure  Accuracy\n",
      "Dataset                        \n",
      "HDFS         0.999984    0.9975\n",
      "Spark        0.991876    0.9200\n",
      "Windows      0.999994    0.9970\n",
      "Apache       1.000000    1.0000\n",
      "Proxifier    0.784886    0.5265\n",
      "OpenSSH      0.999221    0.7875\n",
      "OpenStack    0.992536    0.7325\n",
      "           F1_measure  Accuracy\n",
      "Dataset                        \n",
      "HDFS         0.999990    0.9970\n",
      "Spark        0.991883    0.9220\n",
      "Windows      0.999994    0.9970\n",
      "Apache       1.000000    1.0000\n",
      "Proxifier    0.784655    0.0155\n",
      "OpenSSH      0.912043    0.2315\n",
      "OpenStack    0.992538    0.7215\n",
      "           F1_measure  Accuracy\n",
      "Dataset                        \n",
      "HDFS         0.999990    0.9970\n",
      "Spark        0.841471    0.6175\n",
      "Windows      0.999288    0.6925\n",
      "Apache       1.000000    1.0000\n",
      "Proxifier    0.784655    0.0155\n",
      "OpenSSH      0.911616    0.2185\n",
      "OpenStack    0.908658    0.2265\n",
      "           F1_measure  Accuracy\n",
      "Dataset                        \n",
      "HDFS         0.998846    0.8500\n",
      "Spark        0.841471    0.6175\n",
      "Windows      0.999288    0.6925\n",
      "Apache       1.000000    1.0000\n",
      "Proxifier    0.784655    0.0155\n",
      "OpenSSH      0.921900    0.4100\n",
      "OpenStack    0.904903    0.2480\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for dataset, setting in benchmark_settings.items():\n",
    "\n",
    "    print('\\n=== Evaluation on %s ==='%dataset)\n",
    "    indir = os.path.join(input_dir, os.path.dirname(setting['log_file']))\n",
    "    log_file = os.path.basename(setting['log_file'])\n",
    "    try:\n",
    "        parser = model_v1.Drain(log_format=setting['log_format'], rex=setting['regex'], depth=setting['depth']+1, st=setting['st'])\n",
    "        parser.fit(isReconstruct=True,inputFile=input_dir+setting['log_file'],outputFile=output_dir+log_file + '_structured.csv')\n",
    "    except:\n",
    "        continue\n",
    "    opt = Optimizer()\n",
    "    logClusters = parser.logClusters\n",
    "    print('优化前,聚类数: ',len(logClusters),end=' ')\n",
    "    F1_measure, accuracy = evaluator.evaluate(\n",
    "                       groundtruth=os.path.join(indir, log_file + '_structured.csv'),\n",
    "                       parsedresult=os.path.join(output_dir, log_file + '_structured.csv')\n",
    "                       )\n",
    "    v0_bechmark_result.append([dataset, F1_measure, accuracy])\n",
    "#     printClusters(logClusters)\n",
    "    opt.modify(method='merge_sub_tree',resultFile=output_dir+log_file + '_structured.csv', logparser = parser)\n",
    "    logClusters = parser.logClusters\n",
    "    print('合并子树优化,聚类数: ',len(logClusters),end=' ')\n",
    "    F1_measure, accuracy = evaluator.evaluate(\n",
    "                       groundtruth=os.path.join(indir, log_file + '_structured.csv'),\n",
    "                       parsedresult=os.path.join(output_dir, log_file + '_structured.csv')\n",
    "                       )\n",
    "#     printClusters(logClusters)\n",
    "    # createPlot(drain)\n",
    "    v1_bechmark_result.append([dataset, F1_measure, accuracy])\n",
    "    opt.modify(method = 'seq_dist',resultFile=output_dir+log_file + '_structured.csv',logparser=parser,st = 0.8)\n",
    "    logClusters = parser.logClusters\n",
    "    print('合并聚类优化,聚类数: ',len(logClusters),end=' ')\n",
    "#     printClusters(logClusters)\n",
    "    F1_measure, accuracy = evaluator.evaluate(\n",
    "                           groundtruth=os.path.join(indir, log_file + '_structured.csv'),\n",
    "                           parsedresult=os.path.join(output_dir, log_file + '_structured.csv')\n",
    "                           )\n",
    "    v2_bechmark_result.append([dataset, F1_measure, accuracy])\n",
    "    parser = Drain.LogParser(log_format=setting['log_format'], indir=indir, outdir=output_dir, rex=setting['regex'], depth=setting['depth'], st=setting['st'])\n",
    "    parser.parse(log_file)\n",
    "    logClusters = parser.logCluL\n",
    "    print('原Drain,聚类数: ',len(logClusters),end=' ')\n",
    "    F1_measure, accuracy = evaluator.evaluate(\n",
    "                           groundtruth=os.path.join(indir, log_file + '_structured.csv'),\n",
    "                           parsedresult=os.path.join(output_dir, log_file + '_structured.csv')\n",
    "                           )\n",
    "    origin_bechmark_result.append([dataset, F1_measure, accuracy])\n",
    "    pd.read_csv(log_file + '_structured.csv')\n",
    "    print('真实聚类数: ',)\n",
    "\n",
    "print('\\n=== Overall evaluation results ===')\n",
    "df_result = pd.DataFrame(origin_bechmark_result, columns=['Dataset', 'F1_measure', 'Accuracy'])\n",
    "df_result.set_index('Dataset', inplace=True)\n",
    "print(df_result)\n",
    "df_result = pd.DataFrame(v0_bechmark_result, columns=['Dataset', 'F1_measure', 'Accuracy'])\n",
    "df_result.set_index('Dataset', inplace=True)\n",
    "print(df_result)\n",
    "df_result = pd.DataFrame(v1_bechmark_result, columns=['Dataset', 'F1_measure', 'Accuracy'])\n",
    "df_result.set_index('Dataset', inplace=True)\n",
    "print(df_result)\n",
    "df_result = pd.DataFrame(v2_bechmark_result, columns=['Dataset', 'F1_measure', 'Accuracy'])\n",
    "df_result.set_index('Dataset', inplace=True)\n",
    "print(df_result)\n",
    "df_result.T.to_csv('Drain_bechmark_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {1:2,2:1}\n",
    "b = {1:1}\n",
    "list(set(a.keys()).intersection(b.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "parsing phase\n",
    "'''\n",
    "def parse_log(logPath,parsed_result):\n",
    "\n",
    "    rex = ['blk_(|-)[0-9]+', '(/|)([0-9]+\\.){3}[0-9]+(:[0-9]+|)(:|)']\n",
    "    removeCol = [0,1,2]\n",
    "    myParser = Drain(rex=rex,removeCol=removeCol,st=0.5)\n",
    "    myParser.fit(isReconstruct=True,inputFile=logPath,outputFile=parsed_result)\n",
    "\n",
    "#     myParser.save(savePath=cluster_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load data: 2000it [00:00, 6644.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build the prefix tree process takes 0.3579902648925781\n",
      "the number of log used to build the parser is  2000\n",
      "*********************************************\n"
     ]
    }
   ],
   "source": [
    "parse_log('data/logs/HDFS/HDFS_2k.log','data/logs/result/hdfs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from drain.evaluator import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0000, Recall: 1.0000, F1_measure: 1.0000, Parsing_Accuracy: 0.9970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9999901893648792, 0.997)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate('data/logs/result/hdfs.csv','data/logs/HDFS/HDFS_2k.log_structured.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load data: 2000it [00:00, 6501.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build the prefix tree process takes 0.32255101203918457\n",
      "the number of log used to build the parser is  2000\n",
      "*********************************************\n"
     ]
    }
   ],
   "source": [
    "parse_log('data/logs/Apache/Apache_2k.log','data/logs/result/apache.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0000, Recall: 1.0000, F1_measure: 1.0000, Parsing_Accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate('data/logs/result/apache.csv','data/logs/Apache/Apache_2k.log_structured.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit ('cuda_pytorch': conda)",
   "language": "python",
   "name": "python36764bitcudapytorchconda3e33319a1fef4dc990a9d2f171216946"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
