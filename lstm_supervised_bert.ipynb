{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "num_epochs = 20\n",
    "batch_size = 50\n",
    "input_size = 768\n",
    "model_dir = 'model'\n",
    "window_size = 10\n",
    "num_layers = 2\n",
    "hidden_size = 64\n",
    "file_dir = 'data_supervised'\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventId_to_bert = torch.load(\"../bert/bert_raw_data.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = torch.zeros_like(eventId_to_bert['E5'][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventId_to_bert['E0'] = [[],[padding]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "def generate_bert_dataset(file_dir):\n",
    "    eventId_to_bert = torch.load(\"../bert/bert_raw_data.pth\")\n",
    "    padding = torch.zeros_like(eventId_to_bert['E5'][1][0])\n",
    "    eventId_to_bert['E0'] = [[],[padding]]\n",
    "    sessions = []\n",
    "    labels = []\n",
    "    max_len = 50\n",
    "    normal_data = set()\n",
    "    with open(file_dir+'/normal.csv', 'r') as f:\n",
    "        for ln in f.readlines():\n",
    "            ln = list(map(lambda n: n, map(int, ln.strip().split())))\n",
    "            if len(ln)>50:\n",
    "                continue\n",
    "    #             max_len = max(max_len,len(ln))\n",
    "            normal_data.add(tuple(ln))\n",
    "    abnormal_data = set()\n",
    "    with open(file_dir+'/abnormal.csv', 'r') as f:\n",
    "        for ln in f.readlines():\n",
    "            ln =list(map(lambda n: n, map(int, ln.strip().split())))\n",
    "            if len(ln)>50:\n",
    "                continue\n",
    "    #             max_len = max(max_len,len(ln))\n",
    "            abnormal_data.add(tuple(ln))\n",
    "    print(max_len)\n",
    "    for line in tqdm(normal_data, \"normal:\"):\n",
    "        line = list(line) + [0]*(max_len-len(line))\n",
    "        bert_input = []\n",
    "        for id in line:\n",
    "            bert_input.append(eventId_to_bert['E'+str(id)][1][0].cpu().numpy())\n",
    "        sessions.append(tuple(bert_input))\n",
    "        labels.append(0)\n",
    "    for line in tqdm(abnormal_data, \"abnormal:\"):\n",
    "        line = list(line) + [0]*(max_len-len(line))\n",
    "        bert_input = []\n",
    "        for id in line:\n",
    "            bert_input.append(eventId_to_bert['E'+str(id)][1][0].cpu().numpy())\n",
    "        sessions.append(tuple(bert_input))\n",
    "        labels.append(1)\n",
    "    print('Number of sessions({}): {}'.format(file_dir, len(sessions)))\n",
    "    print('Number of normal sessions: {}'.format(len(normal_data)))\n",
    "    print('Number of abnormal sessions: {}'.format(len(abnormal_data)))\n",
    "    train_x, test_x, train_y, test_y = train_test_split(sessions, labels,test_size=0.3 )\n",
    "    train_data = TensorDataset(torch.tensor(train_x, dtype=torch.float), torch.tensor(train_y))\n",
    "    train_data= DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "    test_data = TensorDataset(torch.tensor(test_x, dtype=torch.float), torch.tensor(test_y))\n",
    "    test_data = DataLoader(test_data, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = []\n",
    "labels = []\n",
    "max_len = 50\n",
    "normal_data = set()\n",
    "with open(file_dir+'/normal.csv', 'r') as f:\n",
    "    for ln in f.readlines():\n",
    "        ln = list(map(lambda n: n, map(int, ln.strip().split())))\n",
    "        if len(ln)>50:\n",
    "            continue\n",
    "#             max_len = max(max_len,len(ln))\n",
    "        normal_data.add(tuple(ln))\n",
    "abnormal_data = set()\n",
    "with open(file_dir+'/abnormal.csv', 'r') as f:\n",
    "    for ln in f.readlines():\n",
    "        ln =list(map(lambda n: n, map(int, ln.strip().split())))\n",
    "        if len(ln)>50:\n",
    "            continue\n",
    "#             max_len = max(max_len,len(ln))\n",
    "        abnormal_data.add(tuple(ln))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normal:: 100%|█████████████████████████████████████████████████████████████████████| 3977/3977 [01:07<00:00, 59.31it/s]\n"
     ]
    }
   ],
   "source": [
    "for line in tqdm(normal_data, \"normal:\"):\n",
    "    line = list(line) + [0]*(max_len-len(line))\n",
    "    bert_input = []\n",
    "    for id in line:\n",
    "        bert_input.append(eventId_to_bert['E'+str(id)][1][0].cpu().numpy())\n",
    "    sessions.append(tuple(bert_input))\n",
    "    labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "abnormal:: 100%|███████████████████████████████████████████████████████████████████| 4110/4110 [01:09<00:00, 59.21it/s]\n"
     ]
    }
   ],
   "source": [
    "for line in tqdm(abnormal_data, \"abnormal:\"):\n",
    "    line = list(line) + [0]*(max_len-len(line))\n",
    "    bert_input = []\n",
    "    for id in line:\n",
    "        bert_input.append(eventId_to_bert['E'+str(id)][1][0].cpu().numpy())\n",
    "    sessions.append(tuple(bert_input))\n",
    "    labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3773"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sessions(data_supervised): 8087\n",
      "Number of normal sessions: 3977\n",
      "Number of abnormal sessions: 4110\n"
     ]
    }
   ],
   "source": [
    "print('Number of sessions({}): {}'.format(file_dir, len(sessions)))\n",
    "print('Number of normal sessions: {}'.format(len(normal_data)))\n",
    "print('Number of abnormal sessions: {}'.format(len(abnormal_data)))\n",
    "train_x, test_x, train_y, test_y = train_test_split(sessions, labels,test_size=0.3 )\n",
    "train_data = TensorDataset(torch.tensor(train_x, dtype=torch.float), torch.tensor(train_y))\n",
    "train_data= DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_data = TensorDataset(torch.tensor(test_x, dtype=torch.float), torch.tensor(test_y))\n",
    "test_data = DataLoader(test_data, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(file_dir):\n",
    "#     eventId_to_bert = torch.load(\"../bert/bert_raw_data.pth\")\n",
    "#     padding = torch.zeros_like(eventId_to_bert['E5'][1][0])\n",
    "#     eventId_to_bert['E0'] = [[],[padding]]\n",
    "    sessions = []\n",
    "    normal_data = set()\n",
    "    labels = []\n",
    "    max_len = 50\n",
    "    with open(file_dir+'/test_normal', 'r') as f:\n",
    "        for ln in f.readlines():\n",
    "            ln = list(map(lambda n: n, map(int, ln.strip().split())))\n",
    "            if len(ln)>50:\n",
    "                continue\n",
    "    #             max_len = max(max_len,len(ln))\n",
    "            normal_data.add(tuple(ln))\n",
    "    abnormal_data = set()\n",
    "    with open(file_dir+'/abnormal', 'r') as f:\n",
    "        for ln in f.readlines():\n",
    "            ln =list(map(lambda n: n, map(int, ln.strip().split())))\n",
    "            if len(ln)>50:\n",
    "                continue\n",
    "    #             max_len = max(max_len,len(ln))\n",
    "            abnormal_data.add(tuple(ln))\n",
    "    print(max_len)\n",
    "    for line in tqdm(normal_data, \"normal:\"):\n",
    "        line = list(line) + [0]*(max_len-len(line))\n",
    "        bert_input = []\n",
    "        for id in line:\n",
    "            bert_input.append(eventId_to_bert['E'+str(id)][1][0].cpu().numpy())\n",
    "        sessions.append(tuple(bert_input))\n",
    "        labels.append(0)\n",
    "    for line in tqdm(abnormal_data, \"abnormal:\"):\n",
    "        line = list(line) + [0]*(max_len-len(line))\n",
    "        bert_input = []\n",
    "        for id in line:\n",
    "            bert_input.append(eventId_to_bert['E'+str(id)][1][0].cpu().numpy())\n",
    "        sessions.append(tuple(bert_input))\n",
    "        labels.append(1)\n",
    "    print('Number of sessions({}): {}'.format(file_dir, len(sessions)))\n",
    "    print('Number of normal sessions: {}'.format(len(normal_data)))\n",
    "    print('Number of abnormal sessions: {}'.format(len(abnormal_data)))\n",
    "    train_x, test_x, train_y, test_y = train_test_split(sessions, labels,test_size=0.2 )\n",
    "    train_data = TensorDataset(torch.tensor(train_x, dtype=torch.float), torch.tensor(train_y))\n",
    "    train_data= DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "    test_data = TensorDataset(torch.tensor(test_x, dtype=torch.float), torch.tensor(test_y))\n",
    "    test_data = DataLoader(test_data, batch_size = batch_size)\n",
    "    return train_data,test_data,train_x,train_y,test_x,test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_keys):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True,bidirectional=True)\n",
    "        self.fc = nn.Linear(2*hidden_size, num_keys)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(2*self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(2*self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x20f40cd24a8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normal::   0%|                                                                       | 3/14161 [00:00<08:34, 27.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normal::  86%|█████████████████████████████████████████████████████████▍         | 12149/14161 [03:13<00:23, 84.00it/s]"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'E265'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-76f24a5e319b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mgenerate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-3a34ed10dc27>\u001b[0m in \u001b[0;36mgenerate_dataset\u001b[1;34m(file_dir)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mbert_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0mbert_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meventId_to_bert\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'E'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbert_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'E265'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "normal::  86%|█████████████████████████████████████████████████████████▍         | 12149/14161 [03:30<00:23, 84.00it/s]"
     ]
    }
   ],
   "source": [
    "train_data,test_data,train_x,train_y,test_x,test_y= generate_dataset(file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_size, hidden_size, num_layers, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (lstm): LSTM(768, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer = Summary# writer(log_dir='log/' + log)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], train_loss: 0.1982\n",
      "Epoch [2/20], train_loss: 0.2042\n",
      "Epoch [3/20], train_loss: 0.2179\n",
      "Epoch [4/20], train_loss: 0.1867\n",
      "Epoch [5/20], train_loss: 0.1856\n",
      "Epoch [6/20], train_loss: 0.1768\n",
      "Epoch [7/20], train_loss: 0.1703\n",
      "Epoch [8/20], train_loss: 0.1925\n",
      "Epoch [9/20], train_loss: 0.1678\n",
      "Epoch [10/20], train_loss: 0.2059\n",
      "Epoch [11/20], train_loss: 0.1969\n",
      "Epoch [12/20], train_loss: 0.1767\n",
      "Epoch [13/20], train_loss: 0.1590\n",
      "Epoch [14/20], train_loss: 0.1691\n",
      "Epoch [15/20], train_loss: 0.1558\n",
      "Epoch [16/20], train_loss: 0.1546\n",
      "Epoch [17/20], train_loss: 0.1526\n",
      "Epoch [18/20], train_loss: 0.1753\n",
      "Epoch [19/20], train_loss: 0.1588\n",
      "Epoch [20/20], train_loss: 0.1442\n",
      "elapsed_time: 115.371s\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_data)\n",
    "start_time = time.time()\n",
    "num_epochs=20\n",
    "for epoch in range(num_epochs):  # Loop over the dataset multiple times\n",
    "    train_loss = 0\n",
    "    for step, (seq, label) in enumerate(train_data):\n",
    "        # Forward pass\n",
    "        seq = seq.clone().detach().view(-1, seq.shape[1], input_size).to(device)\n",
    "        output = model(seq)\n",
    "        loss = criterion(output, label.to(device))\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "#         writer.add_graph(model, seq)\n",
    "    print('Epoch [{}/{}], train_loss: {:.4f}'.format(epoch + 1, num_epochs, train_loss / total_step))\n",
    "#     writer.add_scalar('train_loss'traabsrain_loss / total_step, epoch + 1)\n",
    "elapsed_time = time.time() - start_time\n",
    "print('elapsed_time: {:.3f}s'.format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    return (np.argmax(y_pred.cpu().numpy(),1) == y_true.numpy()).astype('int').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.tensor(train_x, dtype=torch.float).reshape(-1,50,input_size)\n",
    "test_x = torch.tensor(test_x, dtype=torch.float).reshape(-1,50,input_size)\n",
    "train_y = torch.tensor(train_y)\n",
    "test_y = torch.tensor(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.151 accuracy:  0.943\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    epoch_loss = 0\n",
    "    for step, (seq, label) in enumerate(train_data):\n",
    "        seq = seq.clone().detach().view(-1, seq.shape[1], input_size).to(device)\n",
    "        test_output = model(seq.to(device))\n",
    "        if step==0:\n",
    "            output = test_output\n",
    "            labels = label\n",
    "        else:\n",
    "            labels = torch.cat([labels,label],0)\n",
    "            output = torch.cat([output,test_output],0)\n",
    "        epoch_loss += criterion(test_output, label.to(device)).data\n",
    "    epoch_accuracy = accuracy(output, labels)\n",
    "    epoch_loss = epoch_loss/len(train_data)\n",
    "    print('loss: ', round(epoch_loss.item(), 3), 'accuracy: ', round(epoch_accuracy.item(), 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.15 accuracy:  0.943\n"
     ]
    }
   ],
   "source": [
    "epoch_accuracy = accuracy(output, labels)\n",
    "epoch_loss = epoch_loss/len(train_data)\n",
    "print('loss: ', round(epoch_loss.item(), 3), 'accuracy: ', round(epoch_accuracy.item(), 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss:  0.331 test_accuracy:  0.88\n",
      "test_loss:  0.117 test_accuracy:  0.96\n",
      "test_loss:  0.151 test_accuracy:  0.94\n",
      "test_loss:  0.126 test_accuracy:  0.96\n",
      "test_loss:  0.327 test_accuracy:  0.88\n",
      "test_loss:  0.154 test_accuracy:  0.94\n",
      "test_loss:  0.125 test_accuracy:  0.96\n",
      "test_loss:  0.167 test_accuracy:  0.96\n",
      "test_loss:  0.094 test_accuracy:  0.96\n",
      "test_loss:  0.166 test_accuracy:  0.92\n",
      "test_loss:  0.134 test_accuracy:  0.96\n",
      "test_loss:  0.079 test_accuracy:  0.98\n",
      "test_loss:  0.14 test_accuracy:  0.96\n",
      "test_loss:  0.177 test_accuracy:  0.94\n",
      "test_loss:  0.233 test_accuracy:  0.92\n",
      "test_loss:  0.259 test_accuracy:  0.92\n",
      "test_loss:  0.073 test_accuracy:  0.96\n",
      "test_loss:  0.16 test_accuracy:  0.92\n",
      "test_loss:  0.148 test_accuracy:  0.94\n",
      "test_loss:  0.192 test_accuracy:  0.92\n",
      "test_loss:  0.104 test_accuracy:  0.98\n",
      "test_loss:  0.067 test_accuracy:  1.0\n",
      "test_loss:  0.104 test_accuracy:  0.96\n",
      "test_loss:  0.123 test_accuracy:  0.98\n",
      "test_loss:  0.225 test_accuracy:  0.9\n",
      "test_loss:  0.235 test_accuracy:  0.92\n",
      "test_loss:  0.069 test_accuracy:  0.98\n",
      "test_loss:  0.144 test_accuracy:  0.94\n",
      "test_loss:  0.206 test_accuracy:  0.92\n",
      "test_loss:  0.195 test_accuracy:  0.92\n",
      "test_loss:  0.086 test_accuracy:  0.98\n",
      "test_loss:  0.238 test_accuracy:  0.92\n",
      "test_loss:  0.094 test_accuracy:  0.98\n",
      "test_loss:  0.18 test_accuracy:  0.92\n",
      "test_loss:  0.07 test_accuracy:  0.98\n",
      "test_loss:  0.059 test_accuracy:  1.0\n",
      "test_loss:  0.172 test_accuracy:  0.92\n",
      "test_loss:  0.068 test_accuracy:  0.98\n",
      "test_loss:  0.14 test_accuracy:  0.96\n",
      "test_loss:  0.139 test_accuracy:  0.96\n",
      "test_loss:  0.073 test_accuracy:  1.0\n",
      "test_loss:  0.098 test_accuracy:  0.98\n",
      "test_loss:  0.229 test_accuracy:  0.9\n",
      "test_loss:  0.174 test_accuracy:  0.94\n",
      "test_loss:  0.047 test_accuracy:  1.0\n",
      "test_loss:  0.088 test_accuracy:  0.98\n",
      "test_loss:  0.195 test_accuracy:  0.94\n",
      "test_loss:  0.088 test_accuracy:  1.0\n",
      "test_loss:  0.193 test_accuracy:  0.9\n",
      "test_loss:  0.11 test_accuracy:  0.96\n",
      "test_loss:  0.248 test_accuracy:  0.9\n",
      "test_loss:  0.131 test_accuracy:  0.96\n",
      "test_loss:  0.078 test_accuracy:  0.96\n",
      "test_loss:  0.157 test_accuracy:  0.94\n",
      "test_loss:  0.134 test_accuracy:  0.94\n",
      "test_loss:  0.155 test_accuracy:  0.92\n",
      "test_loss:  0.167 test_accuracy:  0.92\n",
      "test_loss:  0.109 test_accuracy:  0.96\n",
      "test_loss:  0.17 test_accuracy:  0.9\n",
      "test_loss:  0.164 test_accuracy:  0.9\n",
      "test_loss:  0.193 test_accuracy:  0.9\n",
      "test_loss:  0.174 test_accuracy:  0.94\n",
      "test_loss:  0.086 test_accuracy:  0.96\n",
      "test_loss:  0.344 test_accuracy:  0.84\n",
      "test_loss:  0.149 test_accuracy:  0.94\n",
      "test_loss:  0.133 test_accuracy:  0.96\n",
      "test_loss:  0.198 test_accuracy:  0.9\n",
      "test_loss:  0.073 test_accuracy:  0.96\n",
      "test_loss:  0.111 test_accuracy:  0.98\n",
      "test_loss:  0.116 test_accuracy:  0.98\n",
      "test_loss:  0.097 test_accuracy:  0.96\n",
      "test_loss:  0.249 test_accuracy:  0.92\n",
      "test_loss:  0.144 test_accuracy:  0.96\n",
      "test_loss:  0.208 test_accuracy:  0.9\n",
      "test_loss:  0.082 test_accuracy:  1.0\n",
      "test_loss:  0.094 test_accuracy:  0.96\n",
      "test_loss:  0.096 test_accuracy:  0.96\n",
      "test_loss:  0.092 test_accuracy:  0.96\n",
      "test_loss:  0.221 test_accuracy:  0.92\n",
      "test_loss:  0.202 test_accuracy:  0.88\n",
      "test_loss:  0.211 test_accuracy:  0.9\n",
      "test_loss:  0.173 test_accuracy:  0.96\n",
      "test_loss:  0.207 test_accuracy:  0.9\n",
      "test_loss:  0.085 test_accuracy:  0.98\n",
      "test_loss:  0.127 test_accuracy:  0.96\n",
      "test_loss:  0.169 test_accuracy:  0.94\n",
      "test_loss:  0.144 test_accuracy:  0.96\n",
      "test_loss:  0.126 test_accuracy:  0.94\n",
      "test_loss:  0.192 test_accuracy:  0.92\n",
      "test_loss:  0.145 test_accuracy:  0.96\n",
      "test_loss:  0.076 test_accuracy:  0.96\n",
      "test_loss:  0.079 test_accuracy:  0.98\n",
      "test_loss:  0.187 test_accuracy:  0.94\n",
      "test_loss:  0.214 test_accuracy:  0.92\n",
      "test_loss:  0.187 test_accuracy:  0.94\n",
      "test_loss:  0.217 test_accuracy:  0.92\n",
      "test_loss:  0.233 test_accuracy:  0.9\n",
      "test_loss:  0.164 test_accuracy:  0.96\n",
      "test_loss:  0.124 test_accuracy:  0.94\n",
      "test_loss:  0.062 test_accuracy:  0.98\n",
      "test_loss:  0.172 test_accuracy:  0.96\n",
      "test_loss:  0.233 test_accuracy:  0.88\n",
      "test_loss:  0.188 test_accuracy:  0.92\n",
      "test_loss:  0.156 test_accuracy:  0.96\n",
      "test_loss:  0.114 test_accuracy:  0.94\n",
      "test_loss:  0.046 test_accuracy:  1.0\n",
      "test_loss:  0.156 test_accuracy:  0.92\n",
      "test_loss:  0.116 test_accuracy:  0.94\n",
      "test_loss:  0.129 test_accuracy:  0.96\n",
      "test_loss:  0.15 test_accuracy:  0.92\n",
      "test_loss:  0.13 test_accuracy:  0.94\n",
      "test_loss:  0.084 test_accuracy:  0.98\n",
      "test_loss:  0.335 test_accuracy:  0.86\n",
      "test_loss:  0.076 test_accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for step, (seq, label) in enumerate(train_data):\n",
    "        seq = seq.clone().detach().view(-1, seq.shape[1], input_size).to(device)\n",
    "        test_output = model(seq.to(device))\n",
    "        epoch_loss = criterion(test_output, label.to(device)).data\n",
    "        epoch_accuracy = accuracy(test_output, label)\n",
    "#         epoch_loss = criterion(train_output, train_y.to(device)).data\n",
    "#         test_output = model(test_x.to(device))\n",
    "#         epoch_test_accuracy = accuracy(test_output, test_y)\n",
    "#         epoch_test_loss = criterion(test_output, test_y.to(device)).data\n",
    "#         print('epoch: ', epoch, 'loss: ', round(epoch_loss.item(), 3), 'accuracy: ', round(epoch_accuracy.item(), 3),\n",
    "        print('test_loss: ', round(epoch_loss.item(), 3), 'test_accuracy: ', round(epoch_accuracy.item(), 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.94 GiB (GPU 0; 4.00 GiB total capacity; 1.80 GiB already allocated; 1.11 GiB free; 1.82 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-3c7362d2ca65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtrain_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mepoch_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Programs\\Anaconda\\envs\\cuda_pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-5ade349127ff>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mh0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mc0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Programs\\Anaconda\\envs\\cuda_pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Programs\\Anaconda\\envs\\cuda_pytorch\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    557\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[1;32m--> 559\u001b[1;33m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[0;32m    560\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.94 GiB (GPU 0; 4.00 GiB total capacity; 1.80 GiB already allocated; 1.11 GiB free; 1.82 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    train_output = model(train_x.to(device))\n",
    "    epoch_accuracy = accuracy(train_output, train_y)\n",
    "    epoch_loss = criterion(train_output, train_y.to(device)).data\n",
    "    test_output = model(test_x.to(device))\n",
    "    epoch_test_accuracy = accuracy(test_output, test_y)\n",
    "    epoch_test_loss = criterion(test_output, test_y.to(device)).data\n",
    "    print('epoch: ', epoch, 'loss: ', round(epoch_loss.item(), 3), 'accuracy: ', round(epoch_accuracy.item(), 3),\n",
    "    'test_loss: ', round(epoch_test_loss.item(), 3), 'test_accuracy: ', round(epoch_test_accuracy.item(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit ('cuda_pytorch': conda)",
   "language": "python",
   "name": "python36764bitcudapytorchconda3e33319a1fef4dc990a9d2f171216946"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
